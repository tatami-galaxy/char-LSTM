{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import string\n",
    "import random\n",
    "from random import randint\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "# Read data into string and seperate train and valid data\n",
    "with open('text8') as file:\n",
    "    data = file.read()\n",
    "valid_size = 1000\n",
    "valid_text = data[:valid_size]\n",
    "train_text = data[valid_size:]\n",
    "train_size = len(train_text)\n",
    "valid_size = len(valid_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "# Utility functions\n",
    "vocab_size = len(string.ascii_lowercase) + 1 # 0 index for ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else: return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "# Generate batches parallely across the text at equal intervals\n",
    "# Each batch contains one character from each of the positions\n",
    "# Positions are updated after generating every batch\n",
    "# The next batch would therefore contain the next characters from all the chosen positions\n",
    "# num_unrollings number of batches are processed at once\n",
    "# Each character is represented as a one hot vector\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset*segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros(shape=(self._batch_size, vocab_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def _next(self):\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (inputW1): Linear(in_features=27, out_features=1024)\n",
       "  (inputU1): Linear(in_features=1024, out_features=1024)\n",
       "  (forgetW1): Linear(in_features=27, out_features=1024)\n",
       "  (forgetU1): Linear(in_features=1024, out_features=1024)\n",
       "  (outputW1): Linear(in_features=27, out_features=1024)\n",
       "  (outputU1): Linear(in_features=1024, out_features=1024)\n",
       "  (cellW1): Linear(in_features=27, out_features=1024)\n",
       "  (cellU1): Linear(in_features=1024, out_features=1024)\n",
       "  (inputW2): Linear(in_features=1024, out_features=1024)\n",
       "  (inputU2): Linear(in_features=1024, out_features=1024)\n",
       "  (forgetW2): Linear(in_features=1024, out_features=1024)\n",
       "  (forgetU2): Linear(in_features=1024, out_features=1024)\n",
       "  (outputW2): Linear(in_features=1024, out_features=1024)\n",
       "  (outputU2): Linear(in_features=1024, out_features=1024)\n",
       "  (cellW2): Linear(in_features=1024, out_features=1024)\n",
       "  (cellU2): Linear(in_features=1024, out_features=1024)\n",
       "  (softW): Linear(in_features=1024, out_features=27)\n",
       "  (dropout): Dropout(p=0)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.FloatTensor\n",
    "hidden_size = 1024 # 1024 for gpu \n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        # LSTM architecture\n",
    "        \n",
    "        # Layer1\n",
    "        self.inputW1 = nn.Linear(vocab_size, hidden_size)\n",
    "        self.inputU1 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.forgetW1 = nn.Linear(vocab_size, hidden_size)\n",
    "        self.forgetU1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.outputW1 = nn.Linear(vocab_size, hidden_size)\n",
    "        self.outputU1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.cellW1 = nn.Linear(vocab_size, hidden_size)\n",
    "        self.cellU1 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Layer2\n",
    "        self.inputW2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.inputU2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.forgetW2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.forgetU2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.outputW2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.outputU2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.cellW2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.cellU2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Softmax weight\n",
    "        self.softW = nn.Linear(hidden_size, vocab_size)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        \n",
    "    def forward(self, one_hot_input, cell_prev, hidden_prev, batch_size):\n",
    "        sig = nn.Sigmoid()\n",
    "        tnh = nn.Tanh()\n",
    "        \n",
    "        # Extract cell and hidden data for each layer\n",
    "        cell_prev = Variable(cell_prev)\n",
    "        hidden_prev = Variable(hidden_prev)\n",
    "        cell_prev1 = cell_prev[:batch_size, :]\n",
    "        hidden_prev1 = hidden_prev[:batch_size, :]\n",
    "        cell_prev2 = cell_prev[batch_size:, :]\n",
    "        hidden_prev2 = hidden_prev[batch_size:, :]\n",
    "        \n",
    "        # Layer 1 computation\n",
    "        input_gate = sig(self.inputW1(one_hot_input) + self.inputU1(hidden_prev1))\n",
    "        forget_gate = sig(self.forgetW1(one_hot_input) + self.forgetU1(hidden_prev1))\n",
    "        output_gate = sig(self.outputW1(one_hot_input) + self.outputU1(hidden_prev1))\n",
    "        update = tnh(self.cellW1(one_hot_input) + self.cellU1(hidden_prev1))\n",
    "        cell1 = (forget_gate * cell_prev1) + (input_gate * update)\n",
    "        hidden1 = output_gate * tnh(cell1)\n",
    "        \n",
    "        # Layer 2 computation\n",
    "        input_gate = sig(self.inputW2(hidden1) + self.inputU2(hidden_prev2))\n",
    "        forget_gate = sig(self.forgetW2(hidden1) + self.forgetU2(hidden_prev2))\n",
    "        output_gate = sig(self.outputW2(hidden1) + self.outputU2(hidden_prev2))\n",
    "        update = tnh(self.cellW2(hidden1) + self.cellU2(hidden_prev2))\n",
    "        cell2 = (forget_gate * cell_prev2) + (input_gate * update)\n",
    "        hidden2 = output_gate * tnh(cell2)\n",
    "        \n",
    "        logits = self.softW(hidden2)\n",
    "        logits = self.dropout(logits)\n",
    "        cell = torch.cat((cell1, cell2), dim=0)\n",
    "        hidden = torch.cat((hidden2, hidden2), dim=0)\n",
    "        \n",
    "        return cell.data, hidden.data, logits\n",
    "\n",
    "lstm = LSTM(vocab_size, hidden_size)\n",
    "lstm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateManager(object):\n",
    "    def __init__(self):\n",
    "        self.hidden_state = torch.zeros(2*batch_size, hidden_size).type(dtype)\n",
    "        self.cell_state = torch.zeros(2*batch_size, hidden_size).type(dtype)\n",
    "        \n",
    "    def save_state(self, cell_state, hidden_state):\n",
    "        self.cell_state = cell_state\n",
    "        self.hidden_state = hidden_state\n",
    "        \n",
    "    def load_state(self):\n",
    "        return self.cell_state, self.hidden_state\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.hidden_state = torch.zeros(2*batch_size, hidden_size).type(dtype)\n",
    "        self.cell_state = torch.zeros(2*batch_size, hidden_size).type(dtype)\n",
    "        \n",
    "sm = StateManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "def train():\n",
    "    cell, hidden = sm.load_state()\n",
    "    batches = train_batches._next()\n",
    "    optimizer.zero_grad()\n",
    "    lstm.zero_grad()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss = 0\n",
    "    for u in range(num_unrollings):\n",
    "        one_hot_input = Variable(torch.from_numpy(batches[u]).type(dtype), requires_grad=False)\n",
    "        # cell, hidden, logits = lstm(one_hot_input, cell, hidden)\n",
    "        cell, hidden, logits = lstm(one_hot_input.cuda(), cell.cuda(), hidden.cuda(), batch_size)\n",
    "        labels = Variable(torch.from_numpy(np.argmax(batches[u+1], axis=1)))\n",
    "        # loss += loss_function(logits, labels)\n",
    "        loss += loss_function(logits.cuda(), labels.cuda())\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(lstm.parameters(), 5)\n",
    "    optimizer.step()\n",
    "    sm.save_state(cell, hidden)\n",
    "    return loss / num_unrollings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling implementation\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "# Sample one element from a distribution assumed to be an array of normalized probabilities\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample():\n",
    "    charid = randint(0, vocab_size-1)\n",
    "    print(id2char(charid), end='')\n",
    "    cell = torch.zeros(2*1, hidden_size).type(dtype)\n",
    "    hidden = torch.zeros(2*1, hidden_size).type(dtype) \n",
    "    soft = nn.Softmax(dim=1)\n",
    "    for i in range(100):\n",
    "        one_hot = torch.zeros(1, vocab_size).type(dtype)\n",
    "        one_hot[0, charid] = 1.0\n",
    "        one_hot = Variable(one_hot, requires_grad=False)\n",
    "        # cell, hidden, logits = lstm(one_hot, cell, hidden)\n",
    "        cell, hidden, logits = lstm(one_hot.cuda(), cell.cuda(), hidden.cuda(), 1)\n",
    "        output = soft(logits)\n",
    "        charid = sample_distribution(output.data[0])\n",
    "        print(id2char(charid), end='')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search implementatioon\n",
    "\n",
    "def beam_search(distributions, beam_size, alog_probs, sequences):\n",
    "    \n",
    "    distributions = torch.abs(torch.log(distributions))\n",
    "    matrix = alog_probs + distributions\n",
    "    indices = np.argsort(matrix.numpy(), axis=None)[:beam_size]\n",
    "    indices = [(i//vocab_size, (i-(i//vocab_size)*vocab_size)) for i in indices]\n",
    "    for i in range(beam_size):\n",
    "        alog_probs[i] = matrix[indices[i][0], indices[i][1]]\n",
    "    seq_ids = [i[0] for i in indices]\n",
    "    char_ids = [i[1] for i in indices]\n",
    "    j = 0\n",
    "    temp = [None] * beam_size\n",
    "    for i in seq_ids:\n",
    "        temp[j] = sequences[i] + ',' + str(char_ids[j])\n",
    "        j += 1\n",
    "    sequences = temp\n",
    "    return alog_probs, sequences, char_ids\n",
    "\n",
    "def sample_beam():\n",
    "    beam_size = 5\n",
    "    sequences = list()\n",
    "    alog_probs = torch.ones(beam_size, 1)\n",
    "    charid = randint(0, vocab_size-1)\n",
    "    for i in range(beam_size):\n",
    "        sequences.append(str(charid))\n",
    "    last_indices = [charid] * beam_size\n",
    "    distributions = torch.zeros(beam_size, vocab_size) \n",
    "    cell = torch.zeros(2*1, hidden_size).type(dtype)\n",
    "    hidden = torch.zeros(2*1, hidden_size).type(dtype) \n",
    "    soft = nn.Softmax(dim=1)\n",
    "    for i in range(100):\n",
    "        for b in range(beam_size):\n",
    "            one_hot = torch.zeros(1, vocab_size).type(dtype)\n",
    "            one_hot[0, last_indices[b]] = 1.0\n",
    "            one_hot = Variable(one_hot, requires_grad=False)\n",
    "            # cell, hidden, logits = lstm(one_hot, cell, hidden)\n",
    "            cell, hidden, logits = lstm(one_hot.cuda(), cell.cuda(), hidden.cuda(), 1)\n",
    "            output = soft(logits)\n",
    "            distributions[b] = output.data\n",
    "        alog_probs, sequences, last_indices = beam_search(distributions, beam_size, alog_probs, sequences)     \n",
    "            \n",
    "    ar = sequences[0].split(',')\n",
    "    ids = [int(i) for i in ar]\n",
    "    for i in ids:\n",
    "        print(id2char(i), end='')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "def valid_perplexity():\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss = 0\n",
    "    cell = torch.zeros(2*1, hidden_size).type(dtype)\n",
    "    hidden = torch.zeros(2*1, hidden_size).type(dtype)\n",
    "    for i in range(valid_size):\n",
    "        batches = valid_batches._next()\n",
    "        one_hot_input = Variable(torch.from_numpy(batches[0]).type(dtype), requires_grad=False)\n",
    "        # cell, hidden, logits = lstm(one_hot_input, cell, hidden)\n",
    "        cell, hidden, logits = lstm(one_hot_input.cuda(), cell.cuda(), hidden.cuda(), 1)\n",
    "        labels = Variable(torch.from_numpy(np.argmax(batches[1], axis=1)))\n",
    "        # loss += loss_function(logits, labels)\n",
    "        loss += loss_function(logits.cuda(), labels.cuda())\n",
    "    return torch.exp(loss / valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 3.292 \n",
      "Minibatch perplexity: 26.895\n",
      "Validation perplexity: 25.347\n",
      "eipygdfcwkmqiifsnsadjhylnfuxtwkjkjregafuxzu ha qlan hsyfzgmwimbacvc rtganyrcus ugrgqvbxsmmsnnrbxxwtvr \n",
      "\n",
      "Average loss at step 1000: 2.300 \n",
      "Minibatch perplexity: 9.976\n",
      "Validation perplexity: 10.495\n",
      "lald twniz of whe praccanthe nene ive fxasd anid en ficone ibnte the ritacal licthtes wotiv s the bo  \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-2c8253510f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average loss at step %d: %.3f '\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-2803c88fec27>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iters = 50001 \n",
    "\n",
    "cudnn.benchmark = True\n",
    "cudnn.fasttest = True\n",
    "\n",
    "for i in range(num_iters):\n",
    "    lstm.train()\n",
    "    l = train()\n",
    "    if i%1000 == 0: \n",
    "        print('Average loss at step %d: %.3f ' % (i,l))\n",
    "        print('Minibatch perplexity: %.3f' % torch.exp(l))\n",
    "        print('Validation perplexity: %.3f' % valid_perplexity())\n",
    "        lstm.eval()\n",
    "        sample()\n",
    "        #sample_beam()\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
