{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import string\n",
    "import random\n",
    "from random import randint\n",
    "# import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "# Read data into string and seperate train and valid data\n",
    "with open('text8') as file:\n",
    "    data = file.read()\n",
    "valid_size = 1000\n",
    "valid_text = data[:valid_size]\n",
    "train_text = data[valid_size:]\n",
    "train_size = len(train_text)\n",
    "valid_size = len(valid_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "# Utility functions\n",
    "vocab_size = len(string.ascii_lowercase) + 1 # 0 index for ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else: return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "# Generate batches parallely across the text at equal intervals\n",
    "# Each batch contains one character from each of the positions\n",
    "# Positions are updated after generating every batch\n",
    "# The next batch would therefore contain the next characters from all the chosen positions\n",
    "# num_unrollings number of batches are processed at once\n",
    "# Each character is represented as a one hot vector\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset*segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros(shape=(self._batch_size, vocab_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def _next(self):\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "hidden_size = 512 # 1024 for gpu \n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        # LSTM architecture\n",
    "        self.inputW = nn.Linear(vocab_size, hidden_size)\n",
    "        self.inputU = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.forgetW = nn.Linear(vocab_size, hidden_size)\n",
    "        self.forgetU = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.outputW = nn.Linear(vocab_size, hidden_size)\n",
    "        self.outputU = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.cellW = nn.Linear(vocab_size, hidden_size)\n",
    "        self.cellU = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.softW = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        \n",
    "    def forward(self, one_hot_input, cell_prev, hidden_prev):\n",
    "        sig = nn.Sigmoid()\n",
    "        tnh = nn.Tanh()\n",
    "        cell_prev = Variable(cell_prev)\n",
    "        hidden_prev = Variable(hidden_prev)\n",
    "        input_gate = sig(self.inputW(one_hot_input) + self.inputU(hidden_prev))\n",
    "        forget_gate = sig(self.forgetW(one_hot_input) + self.forgetU(hidden_prev))\n",
    "        output_gate = sig(self.outputW(one_hot_input) + self.outputU(hidden_prev))\n",
    "        update = tnh(self.cellW(one_hot_input) + self.cellU(hidden_prev))\n",
    "        cell = (forget_gate * cell_prev) + (input_gate * update)\n",
    "        hidden = output_gate * tnh(cell)\n",
    "        logits = self.softW(hidden)\n",
    "        logits = self.dropout(logits)\n",
    "        return cell.data, hidden.data, logits\n",
    "\n",
    "lstm = LSTM(vocab_size, hidden_size)\n",
    "# lstm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateManager(object):\n",
    "    def __init__(self):\n",
    "        self.hidden_state = torch.zeros(batch_size, hidden_size).type(dtype)\n",
    "        self.cell_state = torch.zeros(batch_size, hidden_size).type(dtype)\n",
    "        \n",
    "    def save_state(self, cell_state, hidden_state):\n",
    "        self.cell_state = cell_state\n",
    "        self.hidden_state = hidden_state\n",
    "        \n",
    "    def load_state(self):\n",
    "        return self.cell_state, self.hidden_state\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.hidden_state = torch.zeros(batch_size, hidden_size).type(dtype)\n",
    "        self.cell_state = torch.zeros(batch_size, hidden_size).type(dtype)\n",
    "        \n",
    "sm = StateManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "def train():\n",
    "    cell, hidden = sm.load_state()\n",
    "    batches = train_batches._next()\n",
    "    optimizer.zero_grad()\n",
    "    lstm.zero_grad()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss = 0\n",
    "    for u in range(num_unrollings):\n",
    "        one_hot_input = Variable(torch.from_numpy(batches[u]).type(dtype), requires_grad=False)\n",
    "        cell, hidden, logits = lstm(one_hot_input, cell, hidden)\n",
    "        # cell, hidden, logits = lstm(one_hot_input.cuda(), cell.cuda(), hidden.cuda())\n",
    "        labels = Variable(torch.from_numpy(np.argmax(batches[u+1], axis=1)))\n",
    "        loss += loss_function(logits, labels)\n",
    "        # loss += loss_function(logits.cuda(), labels.cuda())\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(lstm.parameters(), 5)\n",
    "    optimizer.step()\n",
    "    sm.save_state(cell, hidden)\n",
    "    return loss / num_unrollings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random character and feed to model\n",
    "# Take predicted character and feed it back again to generate subsequent characters\n",
    "\n",
    "def beam_search(distributions, beam_size, alog_probs, sequences):\n",
    "    \n",
    "    distribution = torch.abs(torch.log(distributions))\n",
    "    matrix = alog_probs * distributions\n",
    "    indices = np.argsort(matrix.numpy(), axis=None)[:beam_size]\n",
    "    indices = [(i//vocab_size, (i-(i//vocab_size)*vocab_size)) for i in indices]\n",
    "    for i in range(beam_size):\n",
    "        alog_probs[i] = matrix[indices[i][0], indices[i][1]]\n",
    "    seq_ids = [i[0] for i in indices]\n",
    "    char_ids = [i[1] for i in indices]\n",
    "    j = 0\n",
    "    temp = [None] * beam_size\n",
    "    for i in seq_ids:\n",
    "        temp[j] = sequences[i] + ',' + str(char_ids[j])\n",
    "        j += 1\n",
    "    sequences = temp\n",
    "    return alog_probs, sequences, char_ids\n",
    "\n",
    "def sample():\n",
    "    beam_size = 5\n",
    "    sequences = list()\n",
    "    alog_probs = torch.ones(beam_size, 1)\n",
    "    charid = randint(0, vocab_size-1)\n",
    "    for i in range(beam_size):\n",
    "        sequences.append(str(charid))\n",
    "    last_indices = [charid] * beam_size\n",
    "    distributions = torch.zeros(beam_size, vocab_size) \n",
    "    cell = torch.zeros(1, hidden_size).type(dtype)\n",
    "    hidden = torch.zeros(1, hidden_size).type(dtype) \n",
    "    soft = nn.Softmax(dim=1)\n",
    "    for i in range(100):\n",
    "        for b in range(beam_size):\n",
    "            one_hot = torch.zeros(1, vocab_size).type(dtype)\n",
    "            one_hot[0, last_indices[b]] = 1.0\n",
    "            one_hot = Variable(one_hot, requires_grad=False)\n",
    "            cell, hidden, logits = lstm(one_hot, cell, hidden)\n",
    "            # cell, hidden, logits = lstm(one_hot.cuda(), cell.cuda(), hidden.cuda())\n",
    "            output = soft(logits)\n",
    "            distributions[b] = output.data\n",
    "        alog_probs, sequences, last_indices = beam_search(distributions, beam_size, alog_probs, sequences)     \n",
    "            \n",
    "    ar = sequences[0].split(',')\n",
    "    ids = [int(i) for i in ar]\n",
    "    for i in ids:\n",
    "        print(id2char(i), end='')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "def valid_perplexity():\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss = 0\n",
    "    cell = torch.zeros(1, hidden_size).type(dtype)\n",
    "    hidden = torch.zeros(1, hidden_size).type(dtype)\n",
    "    for i in range(valid_size):\n",
    "        batches = valid_batches._next()\n",
    "        one_hot_input = Variable(torch.from_numpy(batches[0]).type(dtype), requires_grad=False)\n",
    "        cell, hidden, logits = lstm(one_hot_input, cell, hidden)\n",
    "        # cell, hidden, logits = lstm(one_hot_input.cuda(), cell.cuda(), hidden.cuda())\n",
    "        labels = Variable(torch.from_numpy(np.argmax(batches[1], axis=1)))\n",
    "        loss += loss_function(logits, labels)\n",
    "        # loss += loss_function(logits.cuda(), labels.cuda())\n",
    "    return torch.exp(loss / valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 3.301 \n",
      "Minibatch perplexity: 27.147\n",
      "Validation perplexity: 24.469\n",
      " kjjjjjjjjjjjjjjjjjjjjjjjjjjjk                      \n",
      "\n",
      "Average loss at step 1000: 2.221 \n",
      "Minibatch perplexity: 9.217\n",
      "Validation perplexity: 9.794\n",
      "ejzzzfzfzfzfzq                                      \n",
      "\n",
      "Average loss at step 2000: 2.201 \n",
      "Minibatch perplexity: 9.035\n",
      "Validation perplexity: 8.703\n",
      "hxxzxxxzzzjz                                        \n",
      "\n",
      "Average loss at step 3000: 2.030 \n",
      "Minibatch perplexity: 7.613\n",
      "Validation perplexity: 7.515\n",
      "uxxxxxxxzxxm                                        \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e3687819a47c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average loss at step %d: %.3f '\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-2e5e93bf5c47>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# loss += loss_function(logits.cuda(), labels.cuda())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iters = 50001 #50001, no dropout, lr=0.1\n",
    "\n",
    "# cudnn.benchmark = True\n",
    "# cudnn.fasttest = True\n",
    "\n",
    "for i in range(num_iters):\n",
    "    lstm.train()\n",
    "    l = train()\n",
    "    if i%1000 == 0: \n",
    "        print('Average loss at step %d: %.3f ' % (i,l))\n",
    "        print('Minibatch perplexity: %.3f' % torch.exp(l))\n",
    "        print('Validation perplexity: %.3f' % valid_perplexity())\n",
    "        lstm.eval()\n",
    "        sample()\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
