{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import string\n",
    "import random\n",
    "from random import randint\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "# Read data into string and seperate train and valid data\n",
    "with open('text8') as file:\n",
    "    data = file.read()\n",
    "valid_size = 1000\n",
    "valid_text = data[:valid_size]\n",
    "train_text = data[valid_size:]\n",
    "train_size = len(train_text)\n",
    "valid_size = len(valid_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "# Utility functions\n",
    "vocab_size = len(string.ascii_lowercase) + 1 # 0 index for ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else: return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "# Generate batches parallely across the text at equal intervals\n",
    "# Each batch contains one character from each of the positions\n",
    "# Positions are updated after generating every batch\n",
    "# The next batch would therefore contain the next characters from all the chosen positions\n",
    "# num_unrollings number of batches are processed at once\n",
    "# Each character is represented as a one hot vector\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset*segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros(shape=(self._batch_size, vocab_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def _next(self):\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (inputW): Linear(in_features=27, out_features=1024)\n",
       "  (inputU): Linear(in_features=1024, out_features=1024)\n",
       "  (forgetW): Linear(in_features=27, out_features=1024)\n",
       "  (forgetU): Linear(in_features=1024, out_features=1024)\n",
       "  (outputW): Linear(in_features=27, out_features=1024)\n",
       "  (outputU): Linear(in_features=1024, out_features=1024)\n",
       "  (cellW): Linear(in_features=27, out_features=1024)\n",
       "  (cellU): Linear(in_features=1024, out_features=1024)\n",
       "  (softW): Linear(in_features=1024, out_features=27)\n",
       "  (dropout): Dropout(p=0)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.FloatTensor\n",
    "hidden_size = 1024 # 1024 for gpu \n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        # LSTM architecture\n",
    "        self.inputW = nn.Linear(vocab_size, hidden_size)\n",
    "        self.inputU = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.forgetW = nn.Linear(vocab_size, hidden_size)\n",
    "        self.forgetU = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.outputW = nn.Linear(vocab_size, hidden_size)\n",
    "        self.outputU = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.cellW = nn.Linear(vocab_size, hidden_size)\n",
    "        self.cellU = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.softW = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        \n",
    "    def forward(self, one_hot_input, cell_prev, hidden_prev):\n",
    "        sig = nn.Sigmoid()\n",
    "        tnh = nn.Tanh()\n",
    "        cell_prev = Variable(cell_prev)\n",
    "        hidden_prev = Variable(hidden_prev)\n",
    "        input_gate = sig(self.inputW(one_hot_input) + self.inputU(hidden_prev))\n",
    "        forget_gate = sig(self.forgetW(one_hot_input) + self.forgetU(hidden_prev))\n",
    "        output_gate = sig(self.outputW(one_hot_input) + self.outputU(hidden_prev))\n",
    "        update = tnh(self.cellW(one_hot_input) + self.cellU(hidden_prev))\n",
    "        cell = (forget_gate * cell_prev) + (input_gate * update)\n",
    "        hidden = output_gate * tnh(cell)\n",
    "        logits = self.softW(hidden)\n",
    "        logits = self.dropout(logits)\n",
    "        return cell.data, hidden.data, logits\n",
    "\n",
    "lstm = LSTM(vocab_size, hidden_size)\n",
    "lstm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateManager(object):\n",
    "    def __init__(self):\n",
    "        self.hidden_state = torch.zeros(batch_size, hidden_size).type(dtype)\n",
    "        self.cell_state = torch.zeros(batch_size, hidden_size).type(dtype)\n",
    "        \n",
    "    def save_state(self, cell_state, hidden_state):\n",
    "        self.cell_state = cell_state\n",
    "        self.hidden_state = hidden_state\n",
    "        \n",
    "    def load_state(self):\n",
    "        return self.cell_state, self.hidden_state\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.hidden_state = torch.zeros(batch_size, hidden_size).type(dtype)\n",
    "        self.cell_state = torch.zeros(batch_size, hidden_size).type(dtype)\n",
    "        \n",
    "sm = StateManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "def train():\n",
    "    cell, hidden = sm.load_state()\n",
    "    batches = train_batches._next()\n",
    "    optimizer.zero_grad()\n",
    "    lstm.zero_grad()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss = 0\n",
    "    for u in range(num_unrollings):\n",
    "        one_hot_input = Variable(torch.from_numpy(batches[u]).type(dtype), requires_grad=False)\n",
    "        # cell, hidden, logits = lstm(one_hot_input, cell, hidden)\n",
    "        cell, hidden, logits = lstm(one_hot_input.cuda(), cell.cuda(), hidden.cuda())\n",
    "        labels = Variable(torch.from_numpy(np.argmax(batches[u+1], axis=1)))\n",
    "        # loss += loss_function(logits, labels)\n",
    "        loss += loss_function(logits.cuda(), labels.cuda())\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(lstm.parameters(), 5)\n",
    "    optimizer.step()\n",
    "    sm.save_state(cell, hidden)\n",
    "    return loss / num_unrollings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sampling implementation\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "#Sample one element from a distribution assumed to be an array of normalized probabilities\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample():\n",
    "    charid = randint(0, vocab_size-1)\n",
    "    print(id2char(charid), end='')\n",
    "    cell = torch.zeros(1, hidden_size).type(dtype)\n",
    "    hidden = torch.zeros(1, hidden_size).type(dtype) \n",
    "    soft = nn.Softmax(dim=1)\n",
    "    for i in range(100):\n",
    "        one_hot = torch.zeros(1, vocab_size).type(dtype)\n",
    "        one_hot[0, charid] = 1.0\n",
    "        one_hot = Variable(one_hot, requires_grad=False)\n",
    "        # cell, hidden, logits = lstm(one_hot, cell, hidden)\n",
    "        cell, hidden, logits = lstm(one_hot.cuda(), cell.cuda(), hidden.cuda())\n",
    "        output = soft(logits)\n",
    "        charid = sample_distribution(output.data[0])\n",
    "        print(id2char(charid), end='')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random character and feed to model\n",
    "# Take predicted character and feed it back again to generate subsequent characters\n",
    "\n",
    "def beam_search(distributions, beam_size, alog_probs, sequences):\n",
    "    \n",
    "    distributions = torch.abs(torch.log(distributions))\n",
    "    matrix = alog_probs + distributions\n",
    "    indices = np.argsort(matrix.numpy(), axis=None)[:beam_size]\n",
    "    indices = [(i//vocab_size, (i-(i//vocab_size)*vocab_size)) for i in indices]\n",
    "    for i in range(beam_size):\n",
    "        alog_probs[i] = matrix[indices[i][0], indices[i][1]]\n",
    "    seq_ids = [i[0] for i in indices]\n",
    "    char_ids = [i[1] for i in indices]\n",
    "    j = 0\n",
    "    temp = [None] * beam_size\n",
    "    for i in seq_ids:\n",
    "        temp[j] = sequences[i] + ',' + str(char_ids[j])\n",
    "        j += 1\n",
    "    sequences = temp\n",
    "    return alog_probs, sequences, char_ids\n",
    "\n",
    "def sample():\n",
    "    beam_size = 5\n",
    "    sequences = list()\n",
    "    alog_probs = torch.ones(beam_size, 1)\n",
    "    charid = randint(0, vocab_size-1)\n",
    "    for i in range(beam_size):\n",
    "        sequences.append(str(charid))\n",
    "    last_indices = [charid] * beam_size\n",
    "    distributions = torch.zeros(beam_size, vocab_size) \n",
    "    cell = torch.zeros(1, hidden_size).type(dtype)\n",
    "    hidden = torch.zeros(1, hidden_size).type(dtype) \n",
    "    soft = nn.Softmax(dim=1)\n",
    "    for i in range(100):\n",
    "        for b in range(beam_size):\n",
    "            one_hot = torch.zeros(1, vocab_size).type(dtype)\n",
    "            one_hot[0, last_indices[b]] = 1.0\n",
    "            one_hot = Variable(one_hot, requires_grad=False)\n",
    "            # cell, hidden, logits = lstm(one_hot, cell, hidden)\n",
    "            cell, hidden, logits = lstm(one_hot.cuda(), cell.cuda(), hidden.cuda())\n",
    "            output = soft(logits)\n",
    "            distributions[b] = output.data\n",
    "        alog_probs, sequences, last_indices = beam_search(distributions, beam_size, alog_probs, sequences)     \n",
    "            \n",
    "    ar = sequences[0].split(',')\n",
    "    ids = [int(i) for i in ar]\n",
    "    for i in ids:\n",
    "        print(id2char(i), end='')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "def valid_perplexity():\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss = 0\n",
    "    cell = torch.zeros(1, hidden_size).type(dtype)\n",
    "    hidden = torch.zeros(1, hidden_size).type(dtype)\n",
    "    for i in range(valid_size):\n",
    "        batches = valid_batches._next()\n",
    "        one_hot_input = Variable(torch.from_numpy(batches[0]).type(dtype), requires_grad=False)\n",
    "        # cell, hidden, logits = lstm(one_hot_input, cell, hidden)\n",
    "        cell, hidden, logits = lstm(one_hot_input.cuda(), cell.cuda(), hidden.cuda())\n",
    "        labels = Variable(torch.from_numpy(np.argmax(batches[1], axis=1)))\n",
    "        # loss += loss_function(logits, labels)\n",
    "        loss += loss_function(logits.cuda(), labels.cuda())\n",
    "    return torch.exp(loss / valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 3.287 \n",
      "Minibatch perplexity: 26.768\n",
      "Validation perplexity: 23.265\n",
      "b                                                                                                     \n",
      "\n",
      "Average loss at step 1000: 2.218 \n",
      "Minibatch perplexity: 9.192\n",
      "Validation perplexity: 9.839\n",
      "n he he th anere or fo fe here t then fo fo he anenen athererinin h f the herenene here the ther f th \n",
      "\n",
      "Average loss at step 2000: 2.222 \n",
      "Minibatch perplexity: 9.224\n",
      "Validation perplexity: 8.936\n",
      "by aronononon aner t for th hex f the fofof zere t ze f throne ze f zeronexix f f foner frenin ore fo \n",
      "\n",
      "Average loss at step 3000: 2.073 \n",
      "Minibatch perplexity: 7.948\n",
      "Validation perplexity: 7.703\n",
      "kenererengreronin onix w anis tine on ere hthenereerereinin t fonere t h an one tex inengenonis o ani \n",
      "\n",
      "Average loss at step 4000: 1.926 \n",
      "Minibatch perplexity: 6.862\n",
      "Validation perplexity: 7.147\n",
      "h w wiger inde ane ze ane se s sthe on onina ine th on th h onin h oreredingalee stindighin thininin  \n",
      "\n",
      "Average loss at step 5000: 1.845 \n",
      "Minibatch perplexity: 6.331\n",
      "Validation perplexity: 6.631\n",
      "nethe we ged s t w thengrenge terer tede s a worengenon th ses ange hethe s an tt w we s w w wal ary  \n",
      "\n",
      "Average loss at step 6000: 1.729 \n",
      "Minibatch perplexity: 5.636\n",
      "Validation perplexity: 6.037\n",
      "be w th ane inonin on won onerore onen o cona sininerathininonthala tanon t on ore s ingon inde oul a \n",
      "\n",
      "Average loss at step 7000: 1.750 \n",
      "Minibatch perplexity: 5.755\n",
      "Validation perplexity: 5.804\n",
      "be wein aleny sen alllenincenen ththed ath gh s inthen wininalily ininge at alanon talyen ge in wnine \n",
      "\n",
      "Average loss at step 8000: 1.880 \n",
      "Minibatch perplexity: 6.551\n",
      "Validation perplexity: 5.720\n",
      " neinin n thin sthan onen dinganan wngone ongn won woninond s onis f th wn wongrinononon wneng th win \n",
      "\n",
      "Average loss at step 9000: 1.691 \n",
      "Minibatch perplexity: 5.424\n",
      "Validation perplexity: 5.258\n",
      "edecereanchales t waned ty on anelen os din ales thas onededere alyonictorarenes ovamin ore onaly omi \n",
      "\n",
      "Average loss at step 10000: 1.768 \n",
      "Minibatch perplexity: 5.861\n",
      "Validation perplexity: 5.176\n",
      "s anenicot wanere ss tharigrcew orearin wan s anorofine arin nergegessorokerengy or wanigeres sthenof \n",
      "\n",
      "Average loss at step 11000: 1.644 \n",
      "Minibatch perplexity: 5.178\n",
      "Validation perplexity: 4.992\n",
      "k win th onigenonitheren o t we ore onengrereris ratterevendenino orimenende f t o orety ath orivenin \n",
      "\n",
      "Average loss at step 12000: 1.581 \n",
      "Minibatch perplexity: 4.859\n",
      "Validation perplexity: 4.806\n",
      "x wenen ale oun alenerade wanininingerallal ingendinanen worinere s testes cedigise alalat ily one o  \n",
      "\n",
      "Average loss at step 13000: 1.658 \n",
      "Minibatch perplexity: 5.251\n",
      "Validation perplexity: 4.805\n",
      "berelan tinelasher wn atola alye ont olye aren n onay we aly sterarigheverickaly s wamedaren tte ict  \n",
      "\n",
      "Average loss at step 14000: 1.724 \n",
      "Minibatch perplexity: 5.604\n",
      "Validation perplexity: 4.692\n",
      "l wonothininergearoure in orared orerex ardethenin oriningrinalt stiseneredenily inenuseline trily ig \n",
      "\n",
      "Average loss at step 15000: 1.513 \n",
      "Minibatch perplexity: 4.540\n",
      "Validation perplexity: 4.747\n",
      "x f t myesel weniged ory omewneve olyoneresererelonex onaringenamed fo on on onor onineninomene t ore \n",
      "\n",
      "Average loss at step 16000: 1.640 \n",
      "Minibatch perplexity: 5.154\n",
      "Validation perplexity: 4.607\n",
      "wer s talilimerer ordinarad s ded a th ste omeryowath wed wisolat thigesses wanere oririg wed sthigen \n",
      "\n",
      "Average loss at step 17000: 1.457 \n",
      "Minibatch perplexity: 4.294\n",
      "Validation perplexity: 4.537\n",
      " wout won ated ttonaly wol atherer wamanura on waninanout s cthonothereranthigeres wanis t wor ty ore \n",
      "\n",
      "Average loss at step 18000: 1.451 \n",
      "Minibatch perplexity: 4.269\n",
      "Validation perplexity: 4.459\n",
      "k won stinered tredinag wes ghinestilyogewhteiseralianonas merghe waredeneng thisisofe se wathnelins  \n",
      "\n",
      "Average loss at step 19000: 1.525 \n",
      "Minibatch perplexity: 4.595\n",
      "Validation perplexity: 4.465\n",
      "inewigarenesthanon ongex bonelyeras ss olinenleioninoneninantines mous tinolyonomere danerelige olis  \n",
      "\n",
      "Average loss at step 20000: 1.555 \n",
      "Minibatch perplexity: 4.736\n",
      "Validation perplexity: 4.408\n",
      "blenchigeved tenengratrinantel oran olesilisthtted orerenerex ithan andedereweararananinondes trs ari \n",
      "\n",
      "Average loss at step 21000: 1.451 \n",
      "Minibatch perplexity: 4.265\n",
      "Validation perplexity: 4.335\n",
      "x wonorerinananenedino s t ord t ccyeted on tarallllinetinongene thin woninonoraranofthoreninare s ff \n",
      "\n",
      "Average loss at step 22000: 1.420 \n",
      "Minibatch perplexity: 4.135\n",
      "Validation perplexity: 4.207\n",
      "ly tisthi aneredenere orinen waneranonex wengorighaconincrler on orenige adex lon re thanesin ws ord  \n",
      "\n",
      "Average loss at step 23000: 1.476 \n",
      "Minibatch perplexity: 4.375\n",
      "Validation perplexity: 3.968\n",
      "ge on thilininanonathely orononolelerc chys s allinelelatut thinthi wonont onel tanerendiathenoninors \n",
      "\n",
      "Average loss at step 24000: 1.507 \n",
      "Minibatch perplexity: 4.512\n",
      "Validation perplexity: 4.056\n",
      "gen wane cogre alyedigrigevere wanconth atotenoligen thinon tre thengarananers orereratininincrir ore \n",
      "\n",
      "Average loss at step 25000: 1.576 \n",
      "Minibatch perplexity: 4.835\n",
      "Validation perplexity: 3.978\n",
      "y oreilyes s inorisp wes s inaneno teglyas anisoffilenomere offtanecechoneneng isthing nine tharippl  \n",
      "\n",
      "Average loss at step 26000: 1.546 \n",
      "Minibatch perplexity: 4.693\n",
      "Validation perplexity: 3.986\n",
      "melined an war ag wane ttyedirerenenigesinen pligrevirew icconspengananes analat w wofononalinag ins  \n",
      "\n",
      "Average loss at step 27000: 1.372 \n",
      "Minibatch perplexity: 3.944\n",
      "Validation perplexity: 3.948\n",
      "x s wilinona o t on wino cino t ly aty timous tatinorertha wanananaly wathadewsth in bine ouninderang \n",
      "\n",
      "Average loss at step 28000: 1.539 \n",
      "Minibatch perplexity: 4.659\n",
      "Validation perplexity: 4.016\n",
      "edes t phines t wir wiginofut onantreringerely ai w ocalas thas tilenathe owathan onon s dotheligele  \n",
      "\n",
      "Average loss at step 29000: 1.457 \n",
      "Minibatch perplexity: 4.292\n",
      "Validation perplexity: 3.976\n",
      "qus oth an on centhacoce othanone alouctinang nace noner otyocowhanolinonotans p oningex andinone bat \n",
      "\n",
      "Average loss at step 30000: 1.524 \n",
      "Minibatch perplexity: 4.590\n",
      "Validation perplexity: 3.996\n",
      "gen waderis ilicor desige ach tedinoninin s anona tedes we o on cintigeron c isthicatesthen tine tis  \n",
      "\n",
      "Average loss at step 31000: 1.419 \n",
      "Minibatch perplexity: 4.133\n",
      "Validation perplexity: 4.027\n",
      "ve wn thanineduronenalinet werangy a smevenorery thyergedonanon matinowalinangrariranones werorend s  \n",
      "\n",
      "Average loss at step 32000: 1.412 \n",
      "Minibatch perplexity: 4.105\n",
      "Validation perplexity: 3.964\n",
      "blardin teranoland tha onex onesent traredered t oly s o tha orineres orin s t thinanofoffthenege ono \n",
      "\n",
      "Average loss at step 33000: 1.444 \n",
      "Minibatch perplexity: 4.235\n",
      "Validation perplexity: 4.040\n",
      "usin chiserare athen wanouss a gedentlypones wanonononononde tat inas mesela an ch ryerininonines on  \n",
      "\n",
      "Average loss at step 34000: 1.400 \n",
      "Minibatch perplexity: 4.055\n",
      "Validation perplexity: 3.908\n",
      "x wrery anere okinenatel thanc alininganononatelyct leliratr th co anganomitharonorerof anoris houror \n",
      "\n",
      "Average loss at step 35000: 1.369 \n",
      "Minibatch perplexity: 3.930\n",
      "Validation perplexity: 3.831\n",
      "porerestinel ar ollin thas weath atharingatinousenerenowenonoronarst th st o on stes alewof asergalin \n",
      "\n",
      "Average loss at step 36000: 1.446 \n",
      "Minibatch perplexity: 4.247\n",
      "Validation perplexity: 3.867\n",
      "t wighiline t s onain atedonorath onothanolanocan wananowa aronata wango anofus tono a a tew ath cham \n",
      "\n",
      "Average loss at step 37000: 1.436 \n",
      "Minibatch perplexity: 4.202\n",
      "Validation perplexity: 3.859\n",
      "houst in anononine wanouniononorelowailenomineronesilatinan wilinon alone inelane inelonges alaranona \n",
      "\n",
      "Average loss at step 38000: 1.446 \n",
      "Minibatch perplexity: 4.245\n",
      "Validation perplexity: 3.872\n",
      "ces ses hana atiredinder rityop w w whede thereanearedutis cous ayolyered s thinoply tinedele s a are \n",
      "\n",
      "Average loss at step 39000: 1.368 \n",
      "Minibatch perplexity: 3.928\n",
      "Validation perplexity: 3.788\n",
      "denoro thol arsthonenathenonenondedinered thetheli aconeselst angldenesethinonthedinonorenatininonan  \n",
      "\n",
      "Average loss at step 40000: 1.371 \n",
      "Minibatch perplexity: 3.940\n",
      "Validation perplexity: 3.717\n",
      "a ainecusst atolea ildellyore ano teamanes ts aminon thalexingerinonevinel orewas thinone s thanoucul \n",
      "\n",
      "Average loss at step 41000: 1.329 \n",
      "Minibatch perplexity: 3.777\n",
      "Validation perplexity: 3.638\n",
      "genor taranon ralyeamathinonaren ous wh alyer sthionimevanan arer athadus walanonon wirathenononenong \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 42000: 1.489 \n",
      "Minibatch perplexity: 4.435\n",
      "Validation perplexity: 3.748\n",
      "kenedes orin therralolat oneredec s aregerareys trin s anereridingenenedd tighanonere on anore ones o \n",
      "\n",
      "Average loss at step 43000: 1.341 \n",
      "Minibatch perplexity: 3.821\n",
      "Validation perplexity: 3.661\n",
      "ky anovinenanthanilix renonthochilintarin sty tenon ve milinanes al teddayes d tusuelyelyonareyoferat \n",
      "\n",
      "Average loss at step 44000: 1.339 \n",
      "Minibatch perplexity: 3.815\n",
      "Validation perplexity: 3.653\n",
      "n werabenanelagrer e ano anothintherongesigellinarelilereryo ar elarenelirerinonanother s an rinereth \n",
      "\n",
      "Average loss at step 45000: 1.276 \n",
      "Minibatch perplexity: 3.581\n",
      "Validation perplexity: 3.635\n",
      "jan oreredi a o tinger one o areanous ouristai oly o s anocoglan ayoneronar s cone anonin w atharindi \n",
      "\n",
      "Average loss at step 46000: 1.426 \n",
      "Minibatch perplexity: 4.164\n",
      "Validation perplexity: 3.658\n",
      "housenedanofon alyered conanenecof olanec a wandalenas wanewan ononindithis senedenofonof thinenedano \n",
      "\n",
      "Average loss at step 47000: 1.401 \n",
      "Minibatch perplexity: 4.060\n",
      "Validation perplexity: 3.682\n",
      "k s terdintex ssedinec ononanenone arede oreredeeris s waly ar anonotelanone s thatolaranofanowinolye \n",
      "\n",
      "Average loss at step 48000: 1.314 \n",
      "Minibatch perplexity: 3.720\n",
      "Validation perplexity: 3.668\n",
      "tigeris imainanedigerm tinondintonomanes smaccis athon walydi wayperenex outhonelanolyen s aror wina  \n",
      "\n",
      "Average loss at step 49000: 1.330 \n",
      "Minibatch perplexity: 3.781\n",
      "Validation perplexity: 3.611\n",
      "zedesthonecamextenenanas soned inaneral binalarethanorixelatesig arinanerest inthanerer fed ig thind  \n",
      "\n",
      "Average loss at step 50000: 1.302 \n",
      "Minibatch perplexity: 3.675\n",
      "Validation perplexity: 3.578\n",
      "geronaresesis thigr ola artildigataindinerinowo w thinorurerat ininononerateccorononofo an w hingeves \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_iters = 50001 #50001, no dropout, lr=0.1\n",
    "\n",
    "cudnn.benchmark = True\n",
    "cudnn.fasttest = True\n",
    "\n",
    "for i in range(num_iters):\n",
    "    lstm.train()\n",
    "    l = train()\n",
    "    if i%1000 == 0: \n",
    "        print('Average loss at step %d: %.3f ' % (i,l))\n",
    "        print('Minibatch perplexity: %.3f' % torch.exp(l))\n",
    "        print('Validation perplexity: %.3f' % valid_perplexity())\n",
    "        lstm.eval()\n",
    "        sample()\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
