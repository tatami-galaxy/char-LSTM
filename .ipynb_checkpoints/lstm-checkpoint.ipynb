{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import string\n",
    "import random\n",
    "from random import randint\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "# Read data into string and seperate train and valid data\n",
    "with open('text8') as file:\n",
    "    data = file.read()\n",
    "valid_size = 1000\n",
    "valid_text = data[:valid_size]\n",
    "train_text = data[valid_size:]\n",
    "train_size = len(train_text)\n",
    "valid_size = len(valid_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "# Utility functions\n",
    "vocab_size = len(string.ascii_lowercase) + 1 # 0 index for ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else: return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "# Generate batches parallely across the text at equal intervals\n",
    "# Each batch contains one character from each of the positions\n",
    "# Positions are updated after generating every batch\n",
    "# The next batch would therefore contain the next characters from all the chosen positions\n",
    "# num_unrollings number of batches are processed at once\n",
    "# Each character is represented as a one hot vector\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset*segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros(shape=(self._batch_size, vocab_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def _next(self):\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (inputW1): Linear(in_features=27, out_features=1024)\n",
       "  (inputU1): Linear(in_features=1024, out_features=1024)\n",
       "  (forgetW1): Linear(in_features=27, out_features=1024)\n",
       "  (forgetU1): Linear(in_features=1024, out_features=1024)\n",
       "  (outputW1): Linear(in_features=27, out_features=1024)\n",
       "  (outputU1): Linear(in_features=1024, out_features=1024)\n",
       "  (cellW1): Linear(in_features=27, out_features=1024)\n",
       "  (cellU1): Linear(in_features=1024, out_features=1024)\n",
       "  (inputW2): Linear(in_features=1024, out_features=1024)\n",
       "  (inputU2): Linear(in_features=1024, out_features=1024)\n",
       "  (forgetW2): Linear(in_features=1024, out_features=1024)\n",
       "  (forgetU2): Linear(in_features=1024, out_features=1024)\n",
       "  (outputW2): Linear(in_features=1024, out_features=1024)\n",
       "  (outputU2): Linear(in_features=1024, out_features=1024)\n",
       "  (cellW2): Linear(in_features=1024, out_features=1024)\n",
       "  (cellU2): Linear(in_features=1024, out_features=1024)\n",
       "  (softW): Linear(in_features=1024, out_features=27)\n",
       "  (dropout): Dropout(p=0)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.FloatTensor\n",
    "hidden_size = 1024 # 1024 for gpu \n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        # LSTM architecture\n",
    "        \n",
    "        # Layer1\n",
    "        self.inputW1 = nn.Linear(vocab_size, hidden_size)\n",
    "        self.inputU1 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.forgetW1 = nn.Linear(vocab_size, hidden_size)\n",
    "        self.forgetU1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.outputW1 = nn.Linear(vocab_size, hidden_size)\n",
    "        self.outputU1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.cellW1 = nn.Linear(vocab_size, hidden_size)\n",
    "        self.cellU1 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Layer2\n",
    "        self.inputW2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.inputU2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.forgetW2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.forgetU2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.outputW2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.outputU2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.cellW2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.cellU2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Softmax weight\n",
    "        self.softW = nn.Linear(hidden_size, vocab_size)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        \n",
    "    def forward(self, one_hot_input, cell_prev, hidden_prev, batch_size):\n",
    "        sig = nn.Sigmoid()\n",
    "        tnh = nn.Tanh()\n",
    "        \n",
    "        # Extract cell and hidden data for each layer\n",
    "        cell_prev = Variable(cell_prev)\n",
    "        hidden_prev = Variable(hidden_prev)\n",
    "        cell_prev1 = cell_prev[:batch_size, :]\n",
    "        hidden_prev1 = hidden_prev[:batch_size, :]\n",
    "        cell_prev2 = cell_prev[batch_size:, :]\n",
    "        hidden_prev2 = hidden_prev[batch_size:, :]\n",
    "        \n",
    "        # Layer 1 computation\n",
    "        input_gate = sig(self.inputW1(one_hot_input) + self.inputU1(hidden_prev1))\n",
    "        forget_gate = sig(self.forgetW1(one_hot_input) + self.forgetU1(hidden_prev1))\n",
    "        output_gate = sig(self.outputW1(one_hot_input) + self.outputU1(hidden_prev1))\n",
    "        update = tnh(self.cellW1(one_hot_input) + self.cellU1(hidden_prev1))\n",
    "        cell1 = (forget_gate * cell_prev1) + (input_gate * update)\n",
    "        hidden1 = output_gate * tnh(cell1)\n",
    "        \n",
    "        # Layer 2 computation\n",
    "        input_gate = sig(self.inputW2(hidden1) + self.inputU2(hidden_prev2))\n",
    "        forget_gate = sig(self.forgetW2(hidden1) + self.forgetU2(hidden_prev2))\n",
    "        output_gate = sig(self.outputW2(hidden1) + self.outputU2(hidden_prev2))\n",
    "        update = tnh(self.cellW2(hidden1) + self.cellU2(hidden_prev2))\n",
    "        cell2 = (forget_gate * cell_prev2) + (input_gate * update)\n",
    "        hidden2 = output_gate * tnh(cell2)\n",
    "        \n",
    "        logits = self.softW(hidden2)\n",
    "        logits = self.dropout(logits)\n",
    "        cell = torch.cat((cell1, cell2), dim=0)\n",
    "        hidden = torch.cat((hidden2, hidden2), dim=0)\n",
    "        \n",
    "        return cell.data, hidden.data, logits\n",
    "\n",
    "lstm = LSTM(vocab_size, hidden_size)\n",
    "lstm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateManager(object):\n",
    "    def __init__(self):\n",
    "        self.hidden_state = torch.zeros(2*batch_size, hidden_size).type(dtype)\n",
    "        self.cell_state = torch.zeros(2*batch_size, hidden_size).type(dtype)\n",
    "        \n",
    "    def save_state(self, cell_state, hidden_state):\n",
    "        self.cell_state = cell_state\n",
    "        self.hidden_state = hidden_state\n",
    "        \n",
    "    def load_state(self):\n",
    "        return self.cell_state, self.hidden_state\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.hidden_state = torch.zeros(2*batch_size, hidden_size).type(dtype)\n",
    "        self.cell_state = torch.zeros(2*batch_size, hidden_size).type(dtype)\n",
    "        \n",
    "sm = StateManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "def train():\n",
    "    cell, hidden = sm.load_state()\n",
    "    batches = train_batches._next()\n",
    "    optimizer.zero_grad()\n",
    "    lstm.zero_grad()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss = 0\n",
    "    for u in range(num_unrollings):\n",
    "        one_hot_input = Variable(torch.from_numpy(batches[u]).type(dtype), requires_grad=False)\n",
    "        # cell, hidden, logits = lstm(one_hot_input, cell, hidden)\n",
    "        cell, hidden, logits = lstm(one_hot_input.cuda(), cell.cuda(), hidden.cuda(), batch_size)\n",
    "        labels = Variable(torch.from_numpy(np.argmax(batches[u+1], axis=1)))\n",
    "        # loss += loss_function(logits, labels)\n",
    "        loss += loss_function(logits.cuda(), labels.cuda())\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(lstm.parameters(), 5)\n",
    "    optimizer.step()\n",
    "    sm.save_state(cell, hidden)\n",
    "    return loss / num_unrollings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling implementation\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "# Sample one element from a distribution assumed to be an array of normalized probabilities\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample():\n",
    "    charid = randint(0, vocab_size-1)\n",
    "    print(id2char(charid), end='')\n",
    "    cell = torch.zeros(2*1, hidden_size).type(dtype)\n",
    "    hidden = torch.zeros(2*1, hidden_size).type(dtype) \n",
    "    soft = nn.Softmax(dim=1)\n",
    "    for i in range(100):\n",
    "        one_hot = torch.zeros(1, vocab_size).type(dtype)\n",
    "        one_hot[0, charid] = 1.0\n",
    "        one_hot = Variable(one_hot, requires_grad=False)\n",
    "        # cell, hidden, logits = lstm(one_hot, cell, hidden)\n",
    "        cell, hidden, logits = lstm(one_hot.cuda(), cell.cuda(), hidden.cuda(), 1)\n",
    "        output = soft(logits)\n",
    "        charid = sample_distribution(output.data[0])\n",
    "        print(id2char(charid), end='')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search implementatioon\n",
    "\n",
    "def beam_search(distributions, beam_size, alog_probs, sequences):\n",
    "    \n",
    "    distributions = torch.abs(torch.log(distributions))\n",
    "    matrix = alog_probs + distributions\n",
    "    indices = np.argsort(matrix.numpy(), axis=None)[:beam_size]\n",
    "    indices = [(i//vocab_size, (i-(i//vocab_size)*vocab_size)) for i in indices]\n",
    "    for i in range(beam_size):\n",
    "        alog_probs[i] = matrix[indices[i][0], indices[i][1]]\n",
    "    seq_ids = [i[0] for i in indices]\n",
    "    char_ids = [i[1] for i in indices]\n",
    "    j = 0\n",
    "    temp = [None] * beam_size\n",
    "    for i in seq_ids:\n",
    "        temp[j] = sequences[i] + ',' + str(char_ids[j])\n",
    "        j += 1\n",
    "    sequences = temp\n",
    "    return alog_probs, sequences, char_ids\n",
    "\n",
    "def sample_beam():\n",
    "    beam_size = 5\n",
    "    alog_probs = torch.ones(beam_size, 1)\n",
    "    charid = randint(0, vocab_size-1)\n",
    "    sequences = [str(charid)] * beam_size\n",
    "    last_indices = [charid] * beam_size\n",
    "    distributions = torch.zeros(beam_size, vocab_size) \n",
    "    cell = torch.zeros(2*1, hidden_size).type(dtype)\n",
    "    hidden = torch.zeros(2*1, hidden_size).type(dtype) \n",
    "    soft = nn.Softmax(dim=1)\n",
    "    for i in range(100):\n",
    "        for b in range(beam_size):\n",
    "            one_hot = torch.zeros(1, vocab_size).type(dtype)\n",
    "            one_hot[0, last_indices[b]] = 1.0\n",
    "            one_hot = Variable(one_hot, requires_grad=False)\n",
    "            # cell, hidden, logits = lstm(one_hot, cell, hidden)\n",
    "            cell, hidden, logits = lstm(one_hot.cuda(), cell.cuda(), hidden.cuda(), 1)\n",
    "            output = soft(logits)\n",
    "            distributions[b] = output.data\n",
    "        alog_probs, sequences, last_indices = beam_search(distributions, beam_size, alog_probs, sequences)     \n",
    "            \n",
    "    ar = sequences[0].split(',')\n",
    "    ids = [int(i) for i in ar]\n",
    "    for i in ids:\n",
    "        print(id2char(i), end='')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "def valid_perplexity():\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss = 0\n",
    "    cell = torch.zeros(2*1, hidden_size).type(dtype)\n",
    "    hidden = torch.zeros(2*1, hidden_size).type(dtype)\n",
    "    for i in range(valid_size):\n",
    "        batches = valid_batches._next()\n",
    "        one_hot_input = Variable(torch.from_numpy(batches[0]).type(dtype), requires_grad=False)\n",
    "        # cell, hidden, logits = lstm(one_hot_input, cell, hidden)\n",
    "        cell, hidden, logits = lstm(one_hot_input.cuda(), cell.cuda(), hidden.cuda(), 1)\n",
    "        labels = Variable(torch.from_numpy(np.argmax(batches[1], axis=1)))\n",
    "        # loss += loss_function(logits, labels)\n",
    "        loss += loss_function(logits.cuda(), labels.cuda())\n",
    "    return torch.exp(loss / valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Average loss at step 0: 3.297 \n",
      "Minibatch perplexity: 27.018\n",
      "Validation perplexity: 25.538\n",
      "fxqfbojgpaqehrskalgykyeslybybladlpbtteftyyozwhaqdvjqhwfsxp parhvwootjlmuyonjpofxrubc mlxkystjss owzba \n",
      "\n",
      "Average loss at step 1000: 2.305 \n",
      "Minibatch perplexity: 10.028\n",
      "Validation perplexity: 10.602\n",
      "go fha devt n desins mone era whe jad on ote wora thines es nt wuur the thith ende ta turinss cing al \n",
      "\n",
      "Average loss at step 2000: 2.254 \n",
      "Minibatch perplexity: 9.523\n",
      "Validation perplexity: 9.275\n",
      "chrelagb hadt andy the on caped a grower gerijpto buseals samdeis an ane lraprens cand one chepmare p \n",
      "\n",
      "Average loss at step 3000: 2.089 \n",
      "Minibatch perplexity: 8.076\n",
      "Validation perplexity: 7.938\n",
      "wer sive tro the ewens tom noze bistity opither pedewrice in onlich toum vord pooply ledante anderico \n",
      "\n",
      "Average loss at step 4000: 1.909 \n",
      "Minibatch perplexity: 6.748\n",
      "Validation perplexity: 7.152\n",
      "ines bat outherva zreecop offare erus ix deser of nine arl desied and fot foubajs proted hecided hich \n",
      "\n",
      "Average loss at step 5000: 1.797 \n",
      "Minibatch perplexity: 6.029\n",
      "Validation perplexity: 6.659\n",
      "amin a simesh notel canina to eight tuorbries in tive eight three artemintal menmanter on six boone r \n",
      "\n",
      "Average loss at step 6000: 1.685 \n",
      "Minibatch perplexity: 5.391\n",
      "Validation perplexity: 6.036\n",
      " hus pencimately grines for the a suble skymets eaconally claived nor bhcher sceess of grodes alibo t \n",
      "\n",
      "Average loss at step 7000: 1.686 \n",
      "Minibatch perplexity: 5.396\n",
      "Validation perplexity: 5.668\n",
      "urcely ve prescration mivired pherfer nurva mictic reable formide was resurertin paisling to sestics  \n",
      "\n",
      "Average loss at step 8000: 1.817 \n",
      "Minibatch perplexity: 6.156\n",
      "Validation perplexity: 5.342\n",
      "mact to how stry canially one eight one nine nine fiven to n b earl in enampla habine six for eochoni \n",
      "\n",
      "Average loss at step 9000: 1.634 \n",
      "Minibatch perplexity: 5.125\n",
      "Validation perplexity: 4.909\n",
      "sife clartical chariambe nocadies and symperage insuage ox rother ful dequestly confimal effects vora \n",
      "\n",
      "Average loss at step 10000: 1.701 \n",
      "Minibatch perplexity: 5.481\n",
      "Validation perplexity: 4.702\n",
      "ment faman reaspan un epfreted time the ceverateskn as fru mulgion of one ninq five peneration emperi \n",
      "\n",
      "Average loss at step 11000: 1.620 \n",
      "Minibatch perplexity: 5.055\n",
      "Validation perplexity: 4.467\n",
      "x staled dison of nolomices agajand amonk buftided an to repotes by the one six three zero dough indu \n",
      "\n",
      "Average loss at step 12000: 1.481 \n",
      "Minibatch perplexity: 4.397\n",
      "Validation perplexity: 4.361\n",
      "y ahim deasutensa for polmocd stalets injotemnes was two indadiams bainstone inten callebong disha on \n",
      "\n",
      "Average loss at step 13000: 1.580 \n",
      "Minibatch perplexity: 4.854\n",
      "Validation perplexity: 4.243\n",
      "pos a simility mostemptictes basand service of and usia and was nave the teakhor the formane risming  \n",
      "\n",
      "Average loss at step 14000: 1.617 \n",
      "Minibatch perplexity: 5.039\n",
      "Validation perplexity: 4.229\n",
      "link highen also point seemen prications is a counched vouns in catabilion the pentule ajon in this i \n",
      "\n",
      "Average loss at step 15000: 1.396 \n",
      "Minibatch perplexity: 4.037\n",
      "Validation perplexity: 4.185\n",
      "fication is common community triessian of potoo gropben associal used of the ener wrowe qufon by imbo \n",
      "\n",
      "Average loss at step 16000: 1.515 \n",
      "Minibatch perplexity: 4.551\n",
      "Validation perplexity: 4.156\n",
      "lowed men foman s protectory relationshipm one nine five other zermanksmarned harbor jomences though  \n",
      "\n",
      "Average loss at step 17000: 1.372 \n",
      "Minibatch perplexity: 3.944\n",
      "Validation perplexity: 3.987\n",
      "age between the say jerary h one seven beritance of government xnix wars mindred warfashing ardena ou \n",
      "\n",
      "Average loss at step 18000: 1.345 \n",
      "Minibatch perplexity: 3.840\n",
      "Validation perplexity: 4.053\n",
      "y the imcluit again broba student be company to maintain alovet computer to u spik over xaza sapan ho \n",
      "\n",
      "Average loss at step 19000: 1.437 \n",
      "Minibatch perplexity: 4.208\n",
      "Validation perplexity: 3.972\n",
      "ur dortring of the one five vesturing with things depecresus althorgane when held would bothers the n \n",
      "\n",
      "Average loss at step 20000: 1.449 \n",
      "Minibatch perplexity: 4.257\n",
      "Validation perplexity: 3.988\n",
      "hes is which come also wide which itric for the efricate manuari in pagantic was names proteinmly to  \n",
      "\n",
      "Average loss at step 21000: 1.372 \n",
      "Minibatch perplexity: 3.942\n",
      "Validation perplexity: 3.919\n",
      "cs cooser pobus especially each in one nine two simur within the with intendation of a temberates the \n",
      "\n",
      "Average loss at step 22000: 1.343 \n",
      "Minibatch perplexity: 3.832\n",
      "Validation perplexity: 3.614\n",
      "ject assist left f online contraction unjove toger the carrquit houses the word book all movent in th \n",
      "\n",
      "Average loss at step 23000: 1.371 \n",
      "Minibatch perplexity: 3.941\n",
      "Validation perplexity: 3.510\n",
      "cal ones that the unixeralic practions to diego minombostor the newty that he teat sephnised that whi \n",
      "\n",
      "Average loss at step 24000: 1.399 \n",
      "Minibatch perplexity: 4.052\n",
      "Validation perplexity: 3.635\n",
      "urbuer was long proxition of metalary gold perform or an acclorator nota computation frequencient pro \n",
      "\n",
      "Average loss at step 25000: 1.505 \n",
      "Minibatch perplexity: 4.503\n",
      "Validation perplexity: 3.711\n",
      "quort narrowen a new signey and at usually bandaha suhum the ne rades out every smallaborqp and merfi \n",
      "\n",
      "Average loss at step 26000: 1.435 \n",
      "Minibatch perplexity: 4.200\n",
      "Validation perplexity: 3.621\n",
      "ble the microcle following no their monomand in t ensolony in one nine six eight might berley and ele \n",
      "\n",
      "Average loss at step 27000: 1.315 \n",
      "Minibatch perplexity: 3.726\n",
      "Validation perplexity: 3.563\n",
      "utation of celechestic he yleasor of airports seven five twarg and focuse chinten of the operatus ski \n",
      "\n",
      "Average loss at step 28000: 1.421 \n",
      "Minibatch perplexity: 4.143\n",
      "Validation perplexity: 3.646\n",
      "matic putponsy have that enabled exploures and the most televisional of the each stadius such martian \n",
      "\n",
      "Average loss at step 29000: 1.355 \n",
      "Minibatch perplexity: 3.878\n",
      "Validation perplexity: 3.612\n",
      "uments that metal condition rew prohane the hand tool and the civil a genetical website confering the \n",
      "\n",
      "Average loss at step 30000: 1.472 \n",
      "Minibatch perplexity: 4.356\n",
      "Validation perplexity: 3.661\n",
      "ded into although movement is results more accidents business of sape visitar competining dris lade a \n",
      "\n",
      "Average loss at step 31000: 1.273 \n",
      "Minibatch perplexity: 3.572\n",
      "Validation perplexity: 3.621\n",
      "y ideas are more not a secret lange co the puppess by lit in medited a harrow one nine three one thre \n",
      "\n",
      "Average loss at step 32000: 1.285 \n",
      "Minibatch perplexity: 3.613\n",
      "Validation perplexity: 3.609\n",
      "quarters about were glassi quadran fr keemine unlikely extendally where july and personal compasible  \n",
      "\n",
      "Average loss at step 33000: 1.335 \n",
      "Minibatch perplexity: 3.798\n",
      "Validation perplexity: 3.766\n",
      "tering to be the jabs intent anyongue a wrand and the colonial overall all parts of y hista conside t \n",
      "\n",
      "Average loss at step 34000: 1.306 \n",
      "Minibatch perplexity: 3.692\n",
      "Validation perplexity: 3.653\n",
      "gernment its the states by also are tended which mussia great collapse turning trates and it was more \n",
      "\n",
      "Average loss at step 35000: 1.315 \n",
      "Minibatch perplexity: 3.727\n",
      "Validation perplexity: 3.535\n",
      "genue held belog that constant anoncille properties and elphare is dating of gotto century announced  \n",
      "\n",
      "Average loss at step 36000: 1.352 \n",
      "Minibatch perplexity: 3.864\n",
      "Validation perplexity: 3.449\n",
      "stenan and amaj timpotime of the varies languages very fortress role the extension and persequal lula \n",
      "\n",
      "Average loss at step 37000: 1.328 \n",
      "Minibatch perplexity: 3.774\n",
      "Validation perplexity: 3.611\n",
      "ved taked band poliament to the grey was a the found of it is recorded to a culture during to womoss  \n",
      "\n",
      "Average loss at step 38000: 1.364 \n",
      "Minibatch perplexity: 3.914\n",
      "Validation perplexity: 3.529\n",
      "f recrimed sepressed it was by the monage in literate user examples were into greatly pannable some s \n",
      "\n",
      "Average loss at step 39000: 1.317 \n",
      "Minibatch perplexity: 3.732\n",
      "Validation perplexity: 3.417\n",
      "ano to clave a certainly science at they hesharts and aldasses from the last the secule christ of the \n",
      "\n",
      "Average loss at step 40000: 1.317 \n",
      "Minibatch perplexity: 3.732\n",
      "Validation perplexity: 3.486\n",
      "go and which parltful invented one zero regarders no service one nine six one st two gets wen whether \n",
      "\n",
      "Average loss at step 41000: 1.261 \n",
      "Minibatch perplexity: 3.530\n",
      "Validation perplexity: 3.277\n",
      "ker the common instrumentation some callings the blint male times more england up n r second then on  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 42000: 1.392 \n",
      "Minibatch perplexity: 4.024\n",
      "Validation perplexity: 3.435\n",
      "ates on the introduced science and played s sorre of hannward would ring i better matter farely in on \n",
      "\n",
      "Average loss at step 43000: 1.213 \n",
      "Minibatch perplexity: 3.365\n",
      "Validation perplexity: 3.436\n",
      "e history of the her to gave dudies space of homitore was the latter champelless the himaley xames ov \n",
      "\n",
      "Average loss at step 44000: 1.281 \n",
      "Minibatch perplexity: 3.599\n",
      "Validation perplexity: 3.395\n",
      "has influential artistic broaders named by tearly onder corresponds in largely general italy champion \n",
      "\n",
      "Average loss at step 45000: 1.190 \n",
      "Minibatch perplexity: 3.286\n",
      "Validation perplexity: 3.313\n",
      "ded sees of premier blbc q qthmger medites and containing for subjeques destroyed for out of a set of \n",
      "\n",
      "Average loss at step 46000: 1.361 \n",
      "Minibatch perplexity: 3.902\n",
      "Validation perplexity: 3.355\n",
      "vagened in addiated by their element and this criminal covean and mountained to secure primacilla fam \n",
      "\n",
      "Average loss at step 47000: 1.311 \n",
      "Minibatch perplexity: 3.709\n",
      "Validation perplexity: 3.415\n",
      "ans honekkuerc min charaches in b one roomser s jow charekta musical revised virain paradoned shortat \n",
      "\n",
      "Average loss at step 48000: 1.215 \n",
      "Minibatch perplexity: 3.370\n",
      "Validation perplexity: 3.454\n",
      "way kresident s released comes between driven lowing and concepty or paccept therapy than the groups  \n",
      "\n",
      "Average loss at step 49000: 1.242 \n",
      "Minibatch perplexity: 3.461\n",
      "Validation perplexity: 3.379\n",
      "werver and lincoln were city is now unways in addition language at port is irib cases however the fag \n",
      "\n",
      "Average loss at step 50000: 1.227 \n",
      "Minibatch perplexity: 3.410\n",
      "Validation perplexity: 3.373\n",
      "de many other reversing the only glots also shores of the inhtal of the defining gates slightly educa \n",
      "\n",
      "Average loss at step 51000: 1.223 \n",
      "Minibatch perplexity: 3.396\n",
      "Validation perplexity: 3.361\n",
      "ber lolo even extensive income them that were with steribated dombilization of the city is the ntl se \n",
      "\n",
      "Average loss at step 52000: 1.305 \n",
      "Minibatch perplexity: 3.687\n",
      "Validation perplexity: 3.455\n",
      "quired antar it was built was ebided as irison cerrential assistant tanking zisoupy early support one \n",
      "\n",
      "Average loss at step 53000: 1.247 \n",
      "Minibatch perplexity: 3.481\n",
      "Validation perplexity: 3.365\n",
      "f whose who address and allac brush s israelise to the world cast dv stuch he personal kincolds one a \n",
      "\n",
      "Average loss at step 54000: 1.226 \n",
      "Minibatch perplexity: 3.407\n",
      "Validation perplexity: 3.333\n",
      "rafts and some mysbublism mps little of bext and saw you as constituent geol player one six protectio \n",
      "\n",
      "Average loss at step 55000: 1.195 \n",
      "Minibatch perplexity: 3.303\n",
      "Validation perplexity: 3.353\n",
      "g is did belong and crashe s website justified the theoretics whom swerists of jobs different a h ven \n",
      "\n",
      "Average loss at step 56000: 1.367 \n",
      "Minibatch perplexity: 3.923\n",
      "Validation perplexity: 3.333\n",
      "ways pry through city presidence of the most nague six five at new yalence floodies the high language \n",
      "\n",
      "Average loss at step 57000: 1.176 \n",
      "Minibatch perplexity: 3.243\n",
      "Validation perplexity: 3.251\n",
      "e not humilling music help ws distributed two new was his is no perhaps such s perceived there fundin \n",
      "\n",
      "Average loss at step 58000: 1.381 \n",
      "Minibatch perplexity: 3.977\n",
      "Validation perplexity: 3.317\n",
      "question began calls are the associated to thus scores we an artistic teams dogmatic surrous in a wor \n",
      "\n",
      "Average loss at step 59000: 1.293 \n",
      "Minibatch perplexity: 3.643\n",
      "Validation perplexity: 3.299\n",
      "vision humans to many other forces sularies to the presidered with a ferrygl special if am nihennalit \n",
      "\n",
      "Average loss at step 60000: 1.397 \n",
      "Minibatch perplexity: 4.044\n",
      "Validation perplexity: 3.321\n",
      "yandeis from the pai of three one nine seven zero s christianity must praguative development can radi \n",
      "\n",
      "Average loss at step 61000: 1.250 \n",
      "Minibatch perplexity: 3.491\n",
      "Validation perplexity: 3.393\n",
      "ord were now proved valley that gains of include outside with the xysqual extended by a dirtini ethio \n",
      "\n",
      "Average loss at step 62000: 1.276 \n",
      "Minibatch perplexity: 3.581\n",
      "Validation perplexity: 3.316\n",
      "ject of bareac constantine agencies are while the styll tyenanties revolution nations of word b two z \n",
      "\n",
      "Average loss at step 63000: 1.348 \n",
      "Minibatch perplexity: 3.850\n",
      "Validation perplexity: 3.328\n",
      "gasimics french to worsing of other empire rotation pinnous army implan deci in one three zero one si \n",
      "\n",
      "Average loss at step 64000: 1.285 \n",
      "Minibatch perplexity: 3.615\n",
      "Validation perplexity: 3.322\n",
      "ned body estrans and most of the government of two five the period six zero one dusication property t \n",
      "\n",
      "Average loss at step 65000: 1.220 \n",
      "Minibatch perplexity: 3.386\n",
      "Validation perplexity: 3.312\n",
      "iency guaugu of album and not buren on being his led by to death deserted about dissas specific late  \n",
      "\n",
      "Average loss at step 66000: 1.253 \n",
      "Minibatch perplexity: 3.499\n",
      "Validation perplexity: 3.305\n",
      "emes no studies from this than by english listenburg mckicately santenne s scene d c d one nine eight \n",
      "\n",
      "Average loss at step 67000: 1.288 \n",
      "Minibatch perplexity: 3.626\n",
      "Validation perplexity: 3.500\n",
      "x three off and returns to have women and completely and point it dehine and but in words not as purp \n",
      "\n",
      "Average loss at step 68000: 1.223 \n",
      "Minibatch perplexity: 3.397\n",
      "Validation perplexity: 3.483\n",
      "est referred to accuracy of both dfc cinimui for later and the runs she were in eder in version and i \n",
      "\n",
      "Average loss at step 69000: 1.246 \n",
      "Minibatch perplexity: 3.478\n",
      "Validation perplexity: 3.345\n",
      "having designers weased on the blacked her has social busineszes song and only ideology anatomy at it \n",
      "\n",
      "Average loss at step 70000: 1.315 \n",
      "Minibatch perplexity: 3.724\n",
      "Validation perplexity: 3.216\n",
      "f scoted to fend jungrithous and separating a many formation of space as data church entaq support le \n",
      "\n",
      "Average loss at step 71000: 1.409 \n",
      "Minibatch perplexity: 4.092\n",
      "Validation perplexity: 3.266\n",
      "taries interview and laha antiquaimeders it because included explanae which had staff european jews s \n",
      "\n",
      "Average loss at step 72000: 1.159 \n",
      "Minibatch perplexity: 3.188\n",
      "Validation perplexity: 3.283\n",
      "ic like magazine was composed can a horsepower features one ephasio ima hemodik italian party checker \n",
      "\n",
      "Average loss at step 73000: 1.260 \n",
      "Minibatch perplexity: 3.525\n",
      "Validation perplexity: 3.392\n",
      "hann greenberg system is two modes of the bay of originally continuum is assigation huntal addition o \n",
      "\n",
      "Average loss at step 74000: 1.254 \n",
      "Minibatch perplexity: 3.505\n",
      "Validation perplexity: 3.320\n",
      "ien and norsen ways city beccehs and history they copyrion in the neock six general japanese delabary \n",
      "\n",
      "Average loss at step 75000: 1.265 \n",
      "Minibatch perplexity: 3.544\n",
      "Validation perplexity: 3.247\n",
      "noths three three two command for the conference of behind the out of this right suggests respects of \n",
      "\n",
      "Average loss at step 76000: 1.328 \n",
      "Minibatch perplexity: 3.773\n",
      "Validation perplexity: 3.343\n",
      "x alexander in jay helped became multi train orthodox cover author partner penter while from point on \n",
      "\n",
      "Average loss at step 77000: 1.161 \n",
      "Minibatch perplexity: 3.192\n",
      "Validation perplexity: 3.282\n",
      "na if some construction in boolen had also license many and tournament chieflexed in derprises betwee \n",
      "\n",
      "Average loss at step 78000: 1.252 \n",
      "Minibatch perplexity: 3.497\n",
      "Validation perplexity: 3.358\n",
      "quested was missile the issue of instituted appear for it is to blastics are infinitely which it rema \n",
      "\n",
      "Average loss at step 79000: 1.237 \n",
      "Minibatch perplexity: 3.444\n",
      "Validation perplexity: 3.433\n",
      "gario cognate becomes spreturate as well as found that over which have further were released on it st \n",
      "\n",
      "Average loss at step 80000: 1.311 \n",
      "Minibatch perplexity: 3.710\n",
      "Validation perplexity: 3.348\n",
      "ory cities three and birthsills killed then centralized however subject as a diamonds to extan after  \n",
      "\n",
      "Average loss at step 81000: 1.214 \n",
      "Minibatch perplexity: 3.368\n",
      "Validation perplexity: 3.263\n",
      "wes economy held to the epopuesing often an icel births of a simon genres and the monteriol tamin fiv \n",
      "\n",
      "Average loss at step 82000: 1.251 \n",
      "Minibatch perplexity: 3.495\n",
      "Validation perplexity: 3.262\n",
      "ken to litarists and essay sexual part deader when prowing three four after the seats are e the sah m \n",
      "\n",
      "Average loss at step 83000: 1.182 \n",
      "Minibatch perplexity: 3.259\n",
      "Validation perplexity: 3.178\n",
      "ban and sense they amount particles his cospil became just as a paul coastal hunkabasy from the world \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 84000: 1.351 \n",
      "Minibatch perplexity: 3.863\n",
      "Validation perplexity: 3.251\n",
      "king her clouds hammozions called johnny city in the one of each state of gods to sq that is hockey b \n",
      "\n",
      "Average loss at step 85000: 1.268 \n",
      "Minibatch perplexity: 3.555\n",
      "Validation perplexity: 3.182\n",
      "xing landland or much toronto a buildings on resolution of the header script or what he called the fi \n",
      "\n",
      "Average loss at step 86000: 1.259 \n",
      "Minibatch perplexity: 3.524\n",
      "Validation perplexity: 3.237\n",
      "it following days to sherton a total to say believed on space in the feminian attempted the tradition \n",
      "\n",
      "Average loss at step 87000: 1.274 \n",
      "Minibatch perplexity: 3.575\n",
      "Validation perplexity: 3.251\n",
      "n and ve s leep are frac two years to dominated he heard or tooch other country s itsorpherms began a \n",
      "\n",
      "Average loss at step 88000: 1.190 \n",
      "Minibatch perplexity: 3.287\n",
      "Validation perplexity: 3.174\n",
      "ex praised historic to perfect pil in the age recovered on visaved to see adams with luther south slo \n",
      "\n",
      "Average loss at step 89000: 1.166 \n",
      "Minibatch perplexity: 3.209\n",
      "Validation perplexity: 3.199\n",
      "ters from trabance in containers and ideal music red the embris however the modern that such electron \n",
      "\n",
      "Average loss at step 90000: 1.299 \n",
      "Minibatch perplexity: 3.666\n",
      "Validation perplexity: 3.198\n",
      "ine he was the signal normal john advantage of zero four nine three diatonic substance attack to brow \n",
      "\n",
      "Average loss at step 91000: 1.251 \n",
      "Minibatch perplexity: 3.495\n",
      "Validation perplexity: 3.138\n",
      "quest town four zero go review of hacketts at mythmam war to major areas the plenigilants this more s \n",
      "\n",
      "Average loss at step 92000: 1.299 \n",
      "Minibatch perplexity: 3.666\n",
      "Validation perplexity: 3.199\n",
      "tage has cultured washington schradop the following stock steal world war in an advanced the drop veh \n",
      "\n",
      "Average loss at step 93000: 1.412 \n",
      "Minibatch perplexity: 4.105\n",
      "Validation perplexity: 3.170\n",
      "on for all outside ownership could meide he was allocate on intelligent x antionum female can be patt \n",
      "\n",
      "Average loss at step 94000: 1.259 \n",
      "Minibatch perplexity: 3.522\n",
      "Validation perplexity: 3.223\n",
      "s considered by rewarded the mentioned the accepting for a large lands to sent even the world and hig \n",
      "\n",
      "Average loss at step 95000: 1.175 \n",
      "Minibatch perplexity: 3.237\n",
      "Validation perplexity: 3.148\n",
      "dicated and brown of the tony course of continuity in territories comes to give traes with a king see \n",
      "\n",
      "Average loss at step 96000: 1.239 \n",
      "Minibatch perplexity: 3.452\n",
      "Validation perplexity: 3.199\n",
      "phanist policy at cottherab in decembers named will serv can be usually a scening neo the town in jul \n",
      "\n",
      "Average loss at step 97000: 1.320 \n",
      "Minibatch perplexity: 3.742\n",
      "Validation perplexity: 3.213\n",
      "shly have mit have owned witnesses of labor two zero zero chorous baptiste ful quec s geeror and popu \n",
      "\n",
      "Average loss at step 98000: 1.184 \n",
      "Minibatch perplexity: 3.267\n",
      "Validation perplexity: 3.279\n",
      " that the remain visible haley years and prouv would reads a memborphiliation and occupation somewher \n",
      "\n",
      "Average loss at step 99000: 1.214 \n",
      "Minibatch perplexity: 3.367\n",
      "Validation perplexity: 3.216\n",
      "ve their hardware because a memorary in dance europe eight debt moved occasionally meseroner music li \n",
      "\n",
      "Average loss at step 100000: 1.285 \n",
      "Minibatch perplexity: 3.616\n",
      "Validation perplexity: 3.180\n",
      "vis generators have a one three seven six six sic lie deaths of funance of the exchange glengriade he \n",
      "\n",
      "Average loss at step 101000: 1.256 \n",
      "Minibatch perplexity: 3.510\n",
      "Validation perplexity: 3.185\n",
      "winder of your for moby short run but felized sueger that jasqui a and iberian two zero to three lost \n",
      "\n",
      "Average loss at step 102000: 1.213 \n",
      "Minibatch perplexity: 3.363\n",
      "Validation perplexity: 3.151\n",
      " the bread dimension movement jews he connected in one zero five most of age villo s standardical in  \n",
      "\n",
      "Average loss at step 103000: 1.127 \n",
      "Minibatch perplexity: 3.085\n",
      "Validation perplexity: 3.166\n",
      "chic acholiced by trotsky sving like for parliament measurement immunomengence convictions were worke \n",
      "\n",
      "Average loss at step 104000: 1.234 \n",
      "Minibatch perplexity: 3.435\n",
      "Validation perplexity: 3.219\n",
      "t march one nine four press called to bare escaping the shiftwarks was a students access to the same  \n",
      "\n",
      "Average loss at step 105000: 1.240 \n",
      "Minibatch perplexity: 3.455\n",
      "Validation perplexity: 3.214\n",
      "doming exists that the prime which indian institution which have two formal gigaed on pine against th \n",
      "\n",
      "Average loss at step 106000: 1.222 \n",
      "Minibatch perplexity: 3.395\n",
      "Validation perplexity: 3.274\n",
      "at in the method valsician books by david at signature on severe on one political energy and normally \n",
      "\n",
      "Average loss at step 107000: 1.156 \n",
      "Minibatch perplexity: 3.176\n",
      "Validation perplexity: 3.261\n",
      "viscell children american and oil fired is returned to his founded with the one nine three one one al \n",
      "\n",
      "Average loss at step 108000: 1.204 \n",
      "Minibatch perplexity: 3.334\n",
      "Validation perplexity: 3.110\n",
      "jecting is velocite languages athens in francisco big currently upff raided nearly it connection by m \n",
      "\n",
      "Average loss at step 109000: 1.302 \n",
      "Minibatch perplexity: 3.676\n",
      "Validation perplexity: 3.161\n",
      "chel new dinker size waving the steel and rapanework and lucidan of fantifictance with haborcheologic \n",
      "\n",
      "Average loss at step 110000: 1.314 \n",
      "Minibatch perplexity: 3.720\n",
      "Validation perplexity: 3.175\n",
      "rigun chemistry of sampounh quinti nuko backgot na translation to the term common and addwdr puil sum \n",
      "\n",
      "Average loss at step 111000: 1.207 \n",
      "Minibatch perplexity: 3.344\n",
      "Validation perplexity: 3.175\n",
      "ore to its spoken circuits rise this tactified at being his also linked level of person gamecadata wa \n",
      "\n",
      "Average loss at step 112000: 1.131 \n",
      "Minibatch perplexity: 3.100\n",
      "Validation perplexity: 3.068\n",
      "quence of weapon s to atlanta accurate buried features it is true to the front of monaccts president  \n",
      "\n",
      "Average loss at step 113000: 1.364 \n",
      "Minibatch perplexity: 3.911\n",
      "Validation perplexity: 3.118\n",
      "ject start of the cornelia which haek before the life see research this artica network surrounding de \n",
      "\n",
      "Average loss at step 114000: 1.191 \n",
      "Minibatch perplexity: 3.289\n",
      "Validation perplexity: 3.145\n",
      "ore beanning on an ukhai english british class comic still agreed to the trickly meets him a div two  \n",
      "\n",
      "Average loss at step 115000: 1.129 \n",
      "Minibatch perplexity: 3.093\n",
      "Validation perplexity: 3.141\n",
      "erowaken s soldicims of st jeast on the london is rudge thirts and reconstructing once the base since \n",
      "\n",
      "Average loss at step 116000: 1.281 \n",
      "Minibatch perplexity: 3.601\n",
      "Validation perplexity: 3.118\n",
      "ration prepube film resources and number of egypt as a digits to reperto and giving opposing that aff \n",
      "\n",
      "Average loss at step 117000: 1.273 \n",
      "Minibatch perplexity: 3.571\n",
      "Validation perplexity: 3.162\n",
      "us inherent a major adaptable prince of scc case the banufacentra book site and julius electro dynast \n",
      "\n",
      "Average loss at step 118000: 1.280 \n",
      "Minibatch perplexity: 3.595\n",
      "Validation perplexity: 3.177\n",
      "king football times bridge public is mostly details such as minerals sr and good four eight two one o \n",
      "\n",
      "Average loss at step 119000: 1.267 \n",
      "Minibatch perplexity: 3.551\n",
      "Validation perplexity: 3.167\n",
      "zen quan u modin burnhamed caterpil effect or or site is used directly of the countries subtracting t \n",
      "\n",
      "Average loss at step 120000: 1.321 \n",
      "Minibatch perplexity: 3.748\n",
      "Validation perplexity: 3.218\n",
      "quency of mars which can require all form an andrew now has released in one nine seven seven march te \n",
      "\n",
      "Average loss at step 121000: 1.177 \n",
      "Minibatch perplexity: 3.244\n",
      "Validation perplexity: 3.169\n",
      "sterling a vienna of the ommens isbn one seven four the tendence david della it would inventions ther \n",
      "\n",
      "Average loss at step 122000: 1.337 \n",
      "Minibatch perplexity: 3.808\n",
      "Validation perplexity: 3.194\n",
      "er natural ce sporting on the united states of the hast species of dictional areas of full liberature \n",
      "\n",
      "Average loss at step 123000: 1.292 \n",
      "Minibatch perplexity: 3.641\n",
      "Validation perplexity: 3.157\n",
      "jos lites he formed crossroats oriented from orson one nine one nine seven six to one that it was set \n",
      "\n",
      "Average loss at step 124000: 1.247 \n",
      "Minibatch perplexity: 3.480\n",
      "Validation perplexity: 3.138\n",
      "inch secretically filmed his reacast one would be noted in predominants angle sacority of emperor her \n",
      "\n",
      "Average loss at step 125000: 1.233 \n",
      "Minibatch perplexity: 3.431\n",
      "Validation perplexity: 3.163\n",
      "wards pukspere jef j l de r bickarde the church editing tactions as an able to actually lawsuit decad \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 126000: 1.171 \n",
      "Minibatch perplexity: 3.224\n",
      "Validation perplexity: 3.190\n",
      "fin edgar and his first point his third staffed a more born the first settlematologist environments l \n",
      "\n",
      "Average loss at step 127000: 1.101 \n",
      "Minibatch perplexity: 3.006\n",
      "Validation perplexity: 3.159\n",
      "ken morning comeo lines andrew ecceding water stress be publisher had a directors of thus in famous s \n",
      "\n",
      "Average loss at step 128000: 1.203 \n",
      "Minibatch perplexity: 3.329\n",
      "Validation perplexity: 3.198\n",
      "x the crahma or condition non foes for the southern a means thiller s that crime eritreached dispute  \n",
      "\n",
      "Average loss at step 129000: 1.271 \n",
      "Minibatch perplexity: 3.565\n",
      "Validation perplexity: 3.201\n",
      "k in the new centaville film green available on the main says interesting self waves the variable def \n",
      "\n",
      "Average loss at step 130000: 1.161 \n",
      "Minibatch perplexity: 3.193\n",
      "Validation perplexity: 3.084\n",
      "ject magnation the gonzo ensuke the chance of the product hears they had been constens while there is \n",
      "\n",
      "Average loss at step 131000: 1.232 \n",
      "Minibatch perplexity: 3.427\n",
      "Validation perplexity: 3.162\n",
      "inated the pitcher in molaccombusic of biology and seaked that a new character over he won the labour \n",
      "\n",
      "Average loss at step 132000: 1.202 \n",
      "Minibatch perplexity: 3.328\n",
      "Validation perplexity: 3.213\n",
      "phrary from ned no incomedy of size keynes to century by the border ebit deaths of photographic acid  \n",
      "\n",
      "Average loss at step 133000: 1.304 \n",
      "Minibatch perplexity: 3.684\n",
      "Validation perplexity: 3.274\n",
      "ston structures anything and marching ball godaridshir mondollagoa china launch of some ottood howeve \n",
      "\n",
      "Average loss at step 134000: 1.202 \n",
      "Minibatch perplexity: 3.325\n",
      "Validation perplexity: 3.174\n",
      "hancachist he is also keep is two two university central results this translation prices were going t \n",
      "\n",
      "Average loss at step 135000: 1.209 \n",
      "Minibatch perplexity: 3.349\n",
      "Validation perplexity: 3.232\n",
      "phremins reports they would not usually to refer to uses first attached such as an even coordinated i \n",
      "\n",
      "Average loss at step 136000: 1.259 \n",
      "Minibatch perplexity: 3.522\n",
      "Validation perplexity: 3.190\n",
      "dir example prostitutors publication like only the damas colour of the rocaception mass had these pro \n",
      "\n",
      "Average loss at step 137000: 1.200 \n",
      "Minibatch perplexity: 3.321\n",
      "Validation perplexity: 3.143\n",
      "jon as being taing too gram military impairment ascended they carifies army in the nettices of decemb \n",
      "\n",
      "Average loss at step 138000: 1.245 \n",
      "Minibatch perplexity: 3.474\n",
      "Validation perplexity: 3.101\n",
      "rupture duday is referred to intensity was even archive the were and he did with however seeking from \n",
      "\n",
      "Average loss at step 139000: 1.159 \n",
      "Minibatch perplexity: 3.187\n",
      "Validation perplexity: 3.116\n",
      "tenness the spc from the internet syndicater traffis of weisee meterendonely tradiations of the motho \n",
      "\n",
      "Average loss at step 140000: 1.293 \n",
      "Minibatch perplexity: 3.644\n",
      "Validation perplexity: 3.136\n",
      "jecut by the pick oblesheever a noteda three ch m one one cousin agassiz from one eight six seven nin \n",
      "\n",
      "Average loss at step 141000: 1.198 \n",
      "Minibatch perplexity: 3.314\n",
      "Validation perplexity: 3.172\n",
      "ury contemporary medical same they kidd for a legal trapula guest diorello signifying to chai dise fi \n",
      "\n",
      "Average loss at step 142000: 1.284 \n",
      "Minibatch perplexity: 3.611\n",
      "Validation perplexity: 3.091\n",
      "i also from a ban brojzer lex train school causing the egyptians columbia unit impersev advertisabili \n",
      "\n",
      "Average loss at step 143000: 1.101 \n",
      "Minibatch perplexity: 3.008\n",
      "Validation perplexity: 3.186\n",
      "resons would take the remates paul reinforcing serbia deformed plants connecticut freedom missions co \n",
      "\n",
      "Average loss at step 144000: 1.254 \n",
      "Minibatch perplexity: 3.505\n",
      "Validation perplexity: 3.181\n",
      "flect limits consonant richard consonants has a few bit of population is neldes mindf radgenia was fi \n",
      "\n",
      "Average loss at step 145000: 1.113 \n",
      "Minibatch perplexity: 3.043\n",
      "Validation perplexity: 3.128\n",
      "x debridged commentary of southern circuits once double are french would only uswood from territories \n",
      "\n",
      "Average loss at step 146000: 1.259 \n",
      "Minibatch perplexity: 3.524\n",
      "Validation perplexity: 3.147\n",
      "ken to davinsians and many was occupied by a bashete for a class largely similar to school and heaven \n",
      "\n",
      "Average loss at step 147000: 1.153 \n",
      "Minibatch perplexity: 3.168\n",
      "Validation perplexity: 3.156\n",
      "itims tradedsher the motion was to sea glanchd exacts with the sava the ruler valcanism gottestanim d \n",
      "\n",
      "Average loss at step 148000: 1.182 \n",
      "Minibatch perplexity: 3.259\n",
      "Validation perplexity: 3.084\n",
      "heavya consumption of the catholic chu knertuler tonguado guide of jumpen most author butlier river h \n",
      "\n",
      "Average loss at step 149000: 1.140 \n",
      "Minibatch perplexity: 3.128\n",
      "Validation perplexity: 3.013\n",
      "rican occupied and references appellence had occupy the limited poderation however widely true glacie \n",
      "\n",
      "Average loss at step 150000: 1.202 \n",
      "Minibatch perplexity: 3.325\n",
      "Validation perplexity: 3.017\n",
      "ers remained en high two mcg speen caperpilly there at live the king one nine six six one american tr \n",
      "\n",
      "Average loss at step 151000: 1.141 \n",
      "Minibatch perplexity: 3.130\n",
      "Validation perplexity: 3.043\n",
      "urcana device and the judicial reputation to mineral imperialists sentential characteristics equel re \n",
      "\n",
      "Average loss at step 152000: 1.235 \n",
      "Minibatch perplexity: 3.438\n",
      "Validation perplexity: 2.966\n",
      "phicism and a politician gregorian track is freeways gosmothesis series eight nine argymer red songs  \n",
      "\n",
      "Average loss at step 153000: 1.179 \n",
      "Minibatch perplexity: 3.252\n",
      "Validation perplexity: 2.971\n",
      "l addition of the prime marxists all plants a grants history arithmetic of authority programs while t \n",
      "\n",
      "Average loss at step 154000: 1.137 \n",
      "Minibatch perplexity: 3.118\n",
      "Validation perplexity: 3.054\n",
      "ering is german foldo often increase slavic screens one nine eight zero zero one when deliberately an \n",
      "\n",
      "Average loss at step 155000: 1.179 \n",
      "Minibatch perplexity: 3.251\n",
      "Validation perplexity: 3.120\n",
      "hanka pair blackwood s shortened exist whether they had felt using diacy although special condomizato \n",
      "\n",
      "Average loss at step 156000: 1.121 \n",
      "Minibatch perplexity: 3.068\n",
      "Validation perplexity: 3.089\n",
      "car consecrated by all or and bohesmith to have been connected almost film he was to mark the triberi \n",
      "\n",
      "Average loss at step 157000: 1.225 \n",
      "Minibatch perplexity: 3.403\n",
      "Validation perplexity: 2.982\n",
      "cholezin is his flows in the surroundings health curlatusm pilgrime usually eight full irish is now s \n",
      "\n",
      "Average loss at step 158000: 1.195 \n",
      "Minibatch perplexity: 3.302\n",
      "Validation perplexity: 2.959\n",
      "zerkner college in one nine eight eight the wanted to flat the variation was not an opport and politi \n",
      "\n",
      "Average loss at step 159000: 1.197 \n",
      "Minibatch perplexity: 3.309\n",
      "Validation perplexity: 2.883\n",
      "z sverranian representatives and machine and release system towards together but also simply required \n",
      "\n",
      "Epoch 1\n",
      "Average loss at step 0: 1.173 \n",
      "Minibatch perplexity: 3.231\n",
      "Validation perplexity: 2.932\n",
      "zon american mamman practices of the peter british europe lympatia presents andrea adult mean in vent \n",
      "\n",
      "Average loss at step 1000: 1.276 \n",
      "Minibatch perplexity: 3.584\n",
      "Validation perplexity: 2.900\n",
      "chyleros itsurrher of a typical poets i f mi c shead that the bible disk copename would be way free d \n",
      "\n",
      "Average loss at step 2000: 1.228 \n",
      "Minibatch perplexity: 3.415\n",
      "Validation perplexity: 2.899\n",
      "zerb and other originally relations center factor adherents jamotaut or peiro boga schoo computerizat \n",
      "\n",
      "Average loss at step 3000: 1.318 \n",
      "Minibatch perplexity: 3.736\n",
      "Validation perplexity: 2.904\n",
      "namera occur the ibm casco are stresses in c to f hps to flattender s choice act one nine eight eight \n",
      "\n",
      "Average loss at step 4000: 1.227 \n",
      "Minibatch perplexity: 3.412\n",
      "Validation perplexity: 2.908\n",
      "ener j x wish ip and legally did not have been decoded the most cases rather than december two paul a \n",
      "\n",
      "Average loss at step 5000: 1.208 \n",
      "Minibatch perplexity: 3.347\n",
      "Validation perplexity: 2.907\n",
      "bour dover slap one seven two one one zero of the government also contemporary amplify modernists coo \n",
      "\n",
      "Average loss at step 6000: 1.240 \n",
      "Minibatch perplexity: 3.457\n",
      "Validation perplexity: 2.911\n",
      "ravillis st two usa which satirizes to identify the deforminantia in the vowel vulnetin deconstruct f \n",
      "\n",
      "Average loss at step 7000: 1.245 \n",
      "Minibatch perplexity: 3.472\n",
      "Validation perplexity: 2.934\n",
      "ves fahrence beesee cast popularized by the rest of all sometimes unusually this cooking was campaign \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 8000: 1.257 \n",
      "Minibatch perplexity: 3.514\n",
      "Validation perplexity: 2.936\n",
      "n is about the matte e eine jan tv one zero three design guerrilland properlant the remainder of the  \n",
      "\n",
      "Average loss at step 9000: 1.312 \n",
      "Minibatch perplexity: 3.714\n",
      "Validation perplexity: 2.919\n",
      "nomes people since it is a countries who would continue to rome second germany alone american point w \n",
      "\n",
      "Average loss at step 10000: 1.228 \n",
      "Minibatch perplexity: 3.413\n",
      "Validation perplexity: 2.951\n",
      "zenazing as an emperors on worldwide the batman predeced early c earlier than two other subgrouper an \n",
      "\n",
      "Average loss at step 11000: 1.203 \n",
      "Minibatch perplexity: 3.328\n",
      "Validation perplexity: 2.949\n",
      "bert is part of phones locally are part of the friends comet in turkey or confusion skin up ministry  \n",
      "\n",
      "Average loss at step 12000: 1.265 \n",
      "Minibatch perplexity: 3.542\n",
      "Validation perplexity: 2.949\n",
      "me itself asian scene comprehensed by top of there are no swords accepting alogna bdong itselffe one  \n",
      "\n",
      "Average loss at step 13000: 1.264 \n",
      "Minibatch perplexity: 3.541\n",
      "Validation perplexity: 2.937\n",
      "ke s levels reading from a lectures in the medieval public i neues of johnny belt are running compres \n",
      "\n",
      "Average loss at step 14000: 1.143 \n",
      "Minibatch perplexity: 3.137\n",
      "Validation perplexity: 2.923\n",
      "ic livewingendent princeton survived urged windows french acid two zero zero zero is an approximately \n",
      "\n",
      "Average loss at step 15000: 1.209 \n",
      "Minibatch perplexity: 3.350\n",
      "Validation perplexity: 2.930\n",
      "e are it apacorsed atenaes is a number of southeastern golden brown american actor due to medieval an \n",
      "\n",
      "Average loss at step 16000: 1.116 \n",
      "Minibatch perplexity: 3.052\n",
      "Validation perplexity: 2.930\n",
      " around one nine nine three seven five zero games so the british army a somewhat protocol screens on  \n",
      "\n",
      "Average loss at step 17000: 1.209 \n",
      "Minibatch perplexity: 3.351\n",
      "Validation perplexity: 2.932\n",
      "f socialy the wave of a vailand society where shadows and made use patients w which where york relate \n",
      "\n",
      "Average loss at step 18000: 1.186 \n",
      "Minibatch perplexity: 3.273\n",
      "Validation perplexity: 2.912\n",
      "kar o smooth diplomatic frequently occurs with the form en delay the communist poetry khan mayzars an \n",
      "\n",
      "Average loss at step 19000: 1.221 \n",
      "Minibatch perplexity: 3.392\n",
      "Validation perplexity: 2.893\n",
      "x security of bose slocked message burnett from these methion which decided to supposed to confirmed  \n",
      "\n",
      "Average loss at step 20000: 1.088 \n",
      "Minibatch perplexity: 2.967\n",
      "Validation perplexity: 2.861\n",
      "gen who political materials on the pianists noman tan shamanim or in the body vehicle is easily numbe \n",
      "\n",
      "Average loss at step 21000: 1.259 \n",
      "Minibatch perplexity: 3.522\n",
      "Validation perplexity: 2.892\n",
      "g per as jali university production in by his drafts they crisis solely one eight zero and one pages  \n",
      "\n",
      "Average loss at step 22000: 1.191 \n",
      "Minibatch perplexity: 3.290\n",
      "Validation perplexity: 2.883\n",
      "froxi british reint hou was interpreted in reserves house and over one males have tier here for the o \n",
      "\n",
      "Average loss at step 23000: 1.135 \n",
      "Minibatch perplexity: 3.111\n",
      "Validation perplexity: 2.879\n",
      "ura the manner islands and the desire as when its similar to sweden may be expects the offensive mark \n",
      "\n",
      "Average loss at step 24000: 1.174 \n",
      "Minibatch perplexity: 3.236\n",
      "Validation perplexity: 2.906\n",
      "bins effectiveness views such as only two bit latter doing platonised parofisho a two zero zero one s \n",
      "\n",
      "Average loss at step 25000: 1.143 \n",
      "Minibatch perplexity: 3.136\n",
      "Validation perplexity: 2.917\n",
      "bert as army world and slaves of the mitsunts in the house were soon performance to refine victory an \n",
      "\n",
      "Average loss at step 26000: 1.318 \n",
      "Minibatch perplexity: 3.736\n",
      "Validation perplexity: 2.912\n",
      "jok kong sgl in one six nine three eight eight po and r n balland was involved all the list of midhav \n",
      "\n",
      "Average loss at step 27000: 1.183 \n",
      "Minibatch perplexity: 3.265\n",
      "Validation perplexity: 2.923\n",
      "glacies and groups devices used for the first personal temperatures as this meet into the three pyram \n",
      "\n",
      "Average loss at step 28000: 1.138 \n",
      "Minibatch perplexity: 3.120\n",
      "Validation perplexity: 2.927\n",
      "companies unstitutions to this defined in practices he those who compressed and the country nor free  \n",
      "\n",
      "Average loss at step 29000: 1.190 \n",
      "Minibatch perplexity: 3.287\n",
      "Validation perplexity: 2.907\n",
      "lock hunt church special knowledge major civilians zanuareus of full buildings most removed the femal \n",
      "\n",
      "Average loss at step 30000: 1.243 \n",
      "Minibatch perplexity: 3.466\n",
      "Validation perplexity: 2.927\n",
      " surface and in order at semitic since one nine seven nine unlike a beast that prophecyse emescoped m \n",
      "\n",
      "Average loss at step 31000: 1.190 \n",
      "Minibatch perplexity: 3.287\n",
      "Validation perplexity: 2.917\n",
      "ore refined in the internet below the republiking a monton was both body electrons illinois several e \n",
      "\n",
      "Average loss at step 32000: 1.237 \n",
      "Minibatch perplexity: 3.444\n",
      "Validation perplexity: 2.881\n",
      "zenai also standard chafol died in third spectrum with delay the confederacy and thelocetic system is \n",
      "\n",
      "Average loss at step 33000: 1.269 \n",
      "Minibatch perplexity: 3.557\n",
      "Validation perplexity: 2.898\n",
      "even birth time american writers like must be pair court in which would have conceiving the native he \n",
      "\n",
      "Average loss at step 34000: 1.231 \n",
      "Minibatch perplexity: 3.425\n",
      "Validation perplexity: 2.915\n",
      "ves distribution of success to released in the western pzechanically clinton sears basic to numerous  \n",
      "\n",
      "Average loss at step 35000: 1.270 \n",
      "Minibatch perplexity: 3.562\n",
      "Validation perplexity: 2.927\n",
      "quer two zero zero zero slope from the fifth tropics often research compressibly helds and they this  \n",
      "\n",
      "Average loss at step 36000: 1.122 \n",
      "Minibatch perplexity: 3.071\n",
      "Validation perplexity: 2.914\n",
      "tope as jewish cuttier communists black company or hidden and when he is a small progressive an objec \n",
      "\n",
      "Average loss at step 37000: 1.182 \n",
      "Minibatch perplexity: 3.261\n",
      "Validation perplexity: 2.882\n",
      "fish types of chinese science of the colossus will slightly in jews which is a sealogy nation is cons \n",
      "\n",
      "Average loss at step 38000: 1.100 \n",
      "Minibatch perplexity: 3.005\n",
      "Validation perplexity: 2.869\n",
      "ing companies use lultimount kissing to six year in belgal and more significant dutch eventually tele \n",
      "\n",
      "Average loss at step 39000: 1.207 \n",
      "Minibatch perplexity: 3.343\n",
      "Validation perplexity: 2.889\n",
      "biogenics in a son of countries and six one zero to other city s five in the directed and the readest \n",
      "\n",
      "Average loss at step 40000: 1.180 \n",
      "Minibatch perplexity: 3.254\n",
      "Validation perplexity: 2.883\n",
      "si probably in abusanium march power of one zero nine five four drawing audien untick please joho hel \n",
      "\n",
      "Average loss at step 41000: 1.186 \n",
      "Minibatch perplexity: 3.275\n",
      "Validation perplexity: 2.876\n",
      "stake murder bosm ip known stories a elected small process could extension degree of vein in frug lon \n",
      "\n",
      "Average loss at step 42000: 1.318 \n",
      "Minibatch perplexity: 3.737\n",
      "Validation perplexity: 2.894\n",
      "ore and the name wex it is substantine across tim is the one one three zero zero zero km that marriag \n",
      "\n",
      "Average loss at step 43000: 1.210 \n",
      "Minibatch perplexity: 3.352\n",
      "Validation perplexity: 2.891\n",
      "an so can cross or heology because of the version or even is pronunciation the hold growth but that a \n",
      "\n",
      "Average loss at step 44000: 1.083 \n",
      "Minibatch perplexity: 2.952\n",
      "Validation perplexity: 2.906\n",
      "type jews quickly differentiable in one nine six zero s the success of four tons now the january one  \n",
      "\n",
      "Average loss at step 45000: 1.167 \n",
      "Minibatch perplexity: 3.213\n",
      "Validation perplexity: 2.908\n",
      "bourgi s one zero five two zero most requirements members on result of the air for them into one inte \n",
      "\n",
      "Average loss at step 46000: 1.255 \n",
      "Minibatch perplexity: 3.509\n",
      "Validation perplexity: 2.902\n",
      "ura on mathbf city but with a great britain chs then tendencyclardy women from man and seized that th \n",
      "\n",
      "Average loss at step 47000: 1.155 \n",
      "Minibatch perplexity: 3.175\n",
      "Validation perplexity: 2.894\n",
      "ury three four seven three five zero to control of wingest graphics with their timbre tiff for an ert \n",
      "\n",
      "Average loss at step 48000: 1.153 \n",
      "Minibatch perplexity: 3.169\n",
      "Validation perplexity: 2.905\n",
      "statized by base extended access bendions and polioc to g fin mio has brought information that contra \n",
      "\n",
      "Average loss at step 49000: 1.138 \n",
      "Minibatch perplexity: 3.120\n",
      "Validation perplexity: 2.924\n",
      "kenochratn denotes over the teams and the country s but if classical reference from the liberal to cl \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 50000: 1.133 \n",
      "Minibatch perplexity: 3.106\n",
      "Validation perplexity: 2.910\n",
      "cs where he was three in one eight zero six education on how cyclon hits was a sense of the legendary \n",
      "\n",
      "Average loss at step 51000: 1.307 \n",
      "Minibatch perplexity: 3.693\n",
      "Validation perplexity: 2.914\n",
      "jection of the lorian officials therapyish and is among the cycle rio bang william italian murderer n \n",
      "\n",
      "Average loss at step 52000: 1.318 \n",
      "Minibatch perplexity: 3.736\n",
      "Validation perplexity: 2.925\n",
      "quest can be in low friend and is pronounced only any overshoem builby vis on jews enjoyled boundarie \n",
      "\n",
      "Average loss at step 53000: 1.189 \n",
      "Minibatch perplexity: 3.284\n",
      "Validation perplexity: 2.880\n",
      "ph free play of the user apside backs geeli where the level two zero zero zero inche is geographicall \n",
      "\n",
      "Average loss at step 54000: 1.194 \n",
      "Minibatch perplexity: 3.300\n",
      "Validation perplexity: 2.891\n",
      " marmiturgy thick is the founder of the market was carried a program the railier called one populatio \n",
      "\n",
      "Average loss at step 55000: 1.159 \n",
      "Minibatch perplexity: 3.187\n",
      "Validation perplexity: 2.907\n",
      "jective and its centre of put with an ingredient reason has become the age of the country in the narc \n",
      "\n",
      "Average loss at step 56000: 1.200 \n",
      "Minibatch perplexity: 3.321\n",
      "Validation perplexity: 2.897\n",
      "gen the post continues in the two history around marshlavense project was refinely non wellerunin the \n",
      "\n",
      "Average loss at step 57000: 1.212 \n",
      "Minibatch perplexity: 3.362\n",
      "Validation perplexity: 2.906\n",
      " costoria since its republic of contacts nine two until catherine immigrants in one nine nine seven e \n",
      "\n",
      "Average loss at step 58000: 1.136 \n",
      "Minibatch perplexity: 3.114\n",
      "Validation perplexity: 2.902\n",
      "he rejected on one four four two zero three eight people an energy the trade down in their designs in \n",
      "\n",
      "Average loss at step 59000: 1.151 \n",
      "Minibatch perplexity: 3.161\n",
      "Validation perplexity: 2.890\n",
      "graph of the islands of design ten emporum of vertical nova in can guru from the octaveopass was comp \n",
      "\n",
      "Average loss at step 60000: 1.269 \n",
      "Minibatch perplexity: 3.557\n",
      "Validation perplexity: 2.891\n",
      "ed shortly and flew of the piece o harles during labout as a study of slapshreet prominent asserted m \n",
      "\n",
      "Average loss at step 61000: 1.209 \n",
      "Minibatch perplexity: 3.350\n",
      "Validation perplexity: 2.909\n",
      "ite access to be due to estimately revolution you take quickly changed from this creation numbers or  \n",
      "\n",
      "Average loss at step 62000: 1.071 \n",
      "Minibatch perplexity: 2.919\n",
      "Validation perplexity: 2.893\n",
      "ches have been a reputation avoiding when took placements moldavia a year the neighboring or his expl \n",
      "\n",
      "Average loss at step 63000: 1.200 \n",
      "Minibatch perplexity: 3.319\n",
      "Validation perplexity: 2.890\n",
      "net in one nine zero us retirement advertised added to pro songs to ensure of literature sports insta \n",
      "\n",
      "Average loss at step 64000: 1.206 \n",
      "Minibatch perplexity: 3.339\n",
      "Validation perplexity: 2.921\n",
      "f fremenson of petroleum pot in the great union colz war japanese they become rapidly a country broad \n",
      "\n",
      "Average loss at step 65000: 1.298 \n",
      "Minibatch perplexity: 3.662\n",
      "Validation perplexity: 2.930\n",
      "finished max of manner following javide five seven nine pedg garden took el two b two when the ritual \n",
      "\n",
      "Average loss at step 66000: 1.286 \n",
      "Minibatch perplexity: 3.617\n",
      "Validation perplexity: 2.892\n",
      "judice australian entriestaller tune that elhander made penetracient lokism was hot creativity of the \n",
      "\n",
      "Average loss at step 67000: 1.221 \n",
      "Minibatch perplexity: 3.392\n",
      "Validation perplexity: 2.871\n",
      "th and state boundary s name political and invented by some tribal three three seven pladiator geolog \n",
      "\n",
      "Average loss at step 68000: 1.213 \n",
      "Minibatch perplexity: 3.364\n",
      "Validation perplexity: 2.855\n",
      "jects his father at an area logical name apart multiply employers in the impubs under the anniversary \n",
      "\n",
      "Average loss at step 69000: 1.250 \n",
      "Minibatch perplexity: 3.490\n",
      "Validation perplexity: 2.876\n",
      "neysgand co was assigns perceived their official called here stop as such as ground easy on flame on  \n",
      "\n",
      "Average loss at step 70000: 1.218 \n",
      "Minibatch perplexity: 3.382\n",
      "Validation perplexity: 2.889\n",
      "landsiad romanian above added the strong birth theory an army many early varied to hydaz or pressure  \n",
      "\n",
      "Average loss at step 71000: 1.132 \n",
      "Minibatch perplexity: 3.102\n",
      "Validation perplexity: 2.899\n",
      " tools and mostly forever with evidence in under his sorrow of the alliance n roommaget show the firs \n",
      "\n",
      "Average loss at step 72000: 1.126 \n",
      "Minibatch perplexity: 3.082\n",
      "Validation perplexity: 2.924\n",
      "ce case of the home town under gas partius and greworders and bawflace is these until one eight eight \n",
      "\n",
      "Average loss at step 73000: 1.196 \n",
      "Minibatch perplexity: 3.308\n",
      "Validation perplexity: 2.942\n",
      "phens had performs and film play based in santos results are equitable fact that it appointed an harl \n",
      "\n",
      "Average loss at step 74000: 1.191 \n",
      "Minibatch perplexity: 3.291\n",
      "Validation perplexity: 2.927\n",
      "man education january one two six three gm two chapters and party to the situate ayers associated wit \n",
      "\n",
      "Average loss at step 75000: 1.109 \n",
      "Minibatch perplexity: 3.031\n",
      "Validation perplexity: 2.929\n",
      "itarily premiershille chilean swing albums meaning as ten farran zio swaceed ships professional barri \n",
      "\n",
      "Average loss at step 76000: 1.174 \n",
      "Minibatch perplexity: 3.236\n",
      "Validation perplexity: 2.946\n",
      "fficient investigated gerschom congreso act as inductor a homogeneous chapter s days the scientists c \n",
      "\n",
      "Average loss at step 77000: 1.222 \n",
      "Minibatch perplexity: 3.393\n",
      "Validation perplexity: 2.930\n",
      "f december gyb for the labor to no one st navy union de first of the south african born independence  \n",
      "\n",
      "Average loss at step 78000: 1.360 \n",
      "Minibatch perplexity: 3.895\n",
      "Validation perplexity: 2.914\n",
      "edmally harvestes a false xnephes relatives and that anything man their the greek one nine eight seve \n",
      "\n",
      "Average loss at step 79000: 1.225 \n",
      "Minibatch perplexity: 3.403\n",
      "Validation perplexity: 2.915\n",
      "ded markov in the republic of the fbj according to the fill of the henriktn was later london is refer \n",
      "\n",
      "Average loss at step 80000: 1.174 \n",
      "Minibatch perplexity: 3.234\n",
      "Validation perplexity: 2.883\n",
      "fromation deals with the term world peersbios upon the life in the genetic phressioe d of five zero z \n",
      "\n",
      "Average loss at step 81000: 1.234 \n",
      "Minibatch perplexity: 3.435\n",
      "Validation perplexity: 2.866\n",
      "able spirituality the universe which he had been constructed inspired that his events as members rare \n",
      "\n",
      "Average loss at step 82000: 1.197 \n",
      "Minibatch perplexity: 3.310\n",
      "Validation perplexity: 2.861\n",
      "ounds using the levels of europe in tartleto genre rural coast texas the last name of cordobal since  \n",
      "\n",
      "Average loss at step 83000: 1.121 \n",
      "Minibatch perplexity: 3.066\n",
      "Validation perplexity: 2.880\n",
      " a bishop of indoor as brightness re e sounding governs had to the executions details on a so inertia \n",
      "\n",
      "Average loss at step 84000: 1.140 \n",
      "Minibatch perplexity: 3.126\n",
      "Validation perplexity: 2.876\n",
      "jammed to k another lisp lines under saw land it wine of i k zhl l s morgan letter dioxides of the fo \n",
      "\n",
      "Average loss at step 85000: 1.112 \n",
      "Minibatch perplexity: 3.042\n",
      "Validation perplexity: 2.875\n",
      "ville this cell identified that the proven spain in june two zero iraqi areas in a truck with literal \n",
      "\n",
      "Average loss at step 86000: 1.317 \n",
      "Minibatch perplexity: 3.734\n",
      "Validation perplexity: 2.866\n",
      "land flats were odyssed in the war then the third in various the calgary edition against climate this \n",
      "\n",
      "Average loss at step 87000: 1.163 \n",
      "Minibatch perplexity: 3.200\n",
      "Validation perplexity: 2.869\n",
      "and is middle east from a further warh called period june one five zero four zeal and last burt these \n",
      "\n",
      "Average loss at step 88000: 1.227 \n",
      "Minibatch perplexity: 3.410\n",
      "Validation perplexity: 2.872\n",
      "reta public and company for an apparent condition although writers cult and player made demographics  \n",
      "\n",
      "Average loss at step 89000: 1.221 \n",
      "Minibatch perplexity: 3.392\n",
      "Validation perplexity: 2.890\n",
      "ers as was fortunately redwick ordinary main article but such as the eleptilly empire was made enabli \n",
      "\n",
      "Average loss at step 90000: 1.118 \n",
      "Minibatch perplexity: 3.059\n",
      "Validation perplexity: 2.885\n",
      "bes of sulrived from creativity in order to the normal olympic models of insect collection communicat \n",
      "\n",
      "Average loss at step 91000: 1.049 \n",
      "Minibatch perplexity: 2.855\n",
      "Validation perplexity: 2.905\n",
      "mary s one two i ever picturing the greece state covenants montaga these objects not to be true of ar \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 92000: 1.313 \n",
      "Minibatch perplexity: 3.717\n",
      "Validation perplexity: 2.892\n",
      "ba is zoted by beegheg first oppression markupse nicklary population would responsible for fm merchan \n",
      "\n",
      "Average loss at step 93000: 1.218 \n",
      "Minibatch perplexity: 3.379\n",
      "Validation perplexity: 2.897\n",
      "get warrior range from brainnacism theologs gcd nstraize the sheep one four one and monoxymous eagles \n",
      "\n",
      "Average loss at step 94000: 1.154 \n",
      "Minibatch perplexity: 3.171\n",
      "Validation perplexity: 2.906\n",
      "it in by squeezes it is oran under the post cases of the armenia in one eight eight nine the month be \n",
      "\n",
      "Average loss at step 95000: 1.338 \n",
      "Minibatch perplexity: 3.812\n",
      "Validation perplexity: 2.916\n",
      "a the novels that the national slies around form and light address and september two zero zero zero f \n",
      "\n",
      "Average loss at step 96000: 1.183 \n",
      "Minibatch perplexity: 3.264\n",
      "Validation perplexity: 2.928\n",
      "phaleps through organization progressive rotational greisgrassion who have no conscripted with some p \n",
      "\n",
      "Average loss at step 97000: 1.287 \n",
      "Minibatch perplexity: 3.624\n",
      "Validation perplexity: 2.916\n",
      "sabite isla drugs to two zero zero five and elnator l vr sil ivanna attented two world ceps and the o \n",
      "\n",
      "Average loss at step 98000: 1.084 \n",
      "Minibatch perplexity: 2.956\n",
      "Validation perplexity: 2.913\n",
      "quest management shortly by pirently the provincs of floric d and when with the naval council of the  \n",
      "\n",
      "Average loss at step 99000: 1.262 \n",
      "Minibatch perplexity: 3.534\n",
      "Validation perplexity: 2.886\n",
      "way through the bummers of tsb wann perunder ambiguitous and internet higher than the wen discovery c \n",
      "\n",
      "Average loss at step 100000: 1.233 \n",
      "Minibatch perplexity: 3.432\n",
      "Validation perplexity: 2.888\n",
      "ury english a power after the remembransae all woman peace are probably tweins of one challenging of  \n",
      "\n",
      "Average loss at step 101000: 1.136 \n",
      "Minibatch perplexity: 3.114\n",
      "Validation perplexity: 2.911\n",
      "boards started that saw themselves brought for below markov discovered around one eight two eight and \n",
      "\n",
      "Average loss at step 102000: 1.174 \n",
      "Minibatch perplexity: 3.236\n",
      "Validation perplexity: 2.900\n",
      "phism being belonging to divine granddauch of a modification including indeed shortbished mr lemming  \n",
      "\n",
      "Average loss at step 103000: 1.174 \n",
      "Minibatch perplexity: 3.236\n",
      "Validation perplexity: 2.917\n",
      "doment to father of senling plasma include a weigh pl having see it is unicodebram soon by the nation \n",
      "\n",
      "Average loss at step 104000: 1.165 \n",
      "Minibatch perplexity: 3.205\n",
      "Validation perplexity: 2.908\n",
      "front including pills copyright under game payment operating from the fair the large electricity fran \n",
      "\n",
      "Average loss at step 105000: 1.209 \n",
      "Minibatch perplexity: 3.350\n",
      "Validation perplexity: 2.901\n",
      "ourable black usage the para children centuries message of james beauty four eight one one seven eigh \n",
      "\n",
      "Average loss at step 106000: 1.156 \n",
      "Minibatch perplexity: 3.176\n",
      "Validation perplexity: 2.875\n",
      "ray and settled in one seven zero s descendants although the confederate of imperial fitting as many  \n",
      "\n",
      "Average loss at step 107000: 1.215 \n",
      "Minibatch perplexity: 3.369\n",
      "Validation perplexity: 2.867\n",
      "juda is orbit and expelles one nine four th centuries hinduism by note it is the typical common rifle \n",
      "\n",
      "Average loss at step 108000: 1.243 \n",
      "Minibatch perplexity: 3.467\n",
      "Validation perplexity: 2.883\n",
      "oring freedom and taught after both assigning his centris in one nine eight five miles to re military \n",
      "\n",
      "Average loss at step 109000: 1.164 \n",
      "Minibatch perplexity: 3.203\n",
      "Validation perplexity: 2.884\n",
      "f six zero six two one zero zero renova california order east one nine nine nine the text one nine ei \n",
      "\n",
      "Average loss at step 110000: 1.176 \n",
      "Minibatch perplexity: 3.241\n",
      "Validation perplexity: 2.876\n",
      "der canon afrikaans during that an early contemporary sir roll brokers cosmomorated to the current ca \n",
      "\n",
      "Average loss at step 111000: 1.285 \n",
      "Minibatch perplexity: 3.614\n",
      "Validation perplexity: 2.892\n",
      "hausmanian seventh australia to becomes a different forr to their endorming liberal of the modern rec \n",
      "\n",
      "Average loss at step 112000: 1.090 \n",
      "Minibatch perplexity: 2.975\n",
      "Validation perplexity: 2.917\n",
      "vanized cinderella s music cities might be desire survivors and legal systems would later people agav \n",
      "\n",
      "Average loss at step 113000: 1.175 \n",
      "Minibatch perplexity: 3.238\n",
      "Validation perplexity: 2.906\n",
      "hindi dynasty and ashes which includes sea electrical governorate standards a continuation within thi \n",
      "\n",
      "Average loss at step 114000: 1.230 \n",
      "Minibatch perplexity: 3.421\n",
      "Validation perplexity: 2.915\n",
      "x jom j y one zero five cambridge conan cuban associated with a term pressure and with the latter or  \n",
      "\n",
      "Average loss at step 115000: 1.229 \n",
      "Minibatch perplexity: 3.419\n",
      "Validation perplexity: 2.912\n",
      "parenageonete also ascent this embryoded humor richard harrison at a she mcg winning her learned his  \n",
      "\n",
      "Average loss at step 116000: 1.357 \n",
      "Minibatch perplexity: 3.884\n",
      "Validation perplexity: 2.912\n",
      "kart on what oil family humorist james have defise synoded to killed sansha that nothing elson to pri \n",
      "\n",
      "Average loss at step 117000: 1.293 \n",
      "Minibatch perplexity: 3.645\n",
      "Validation perplexity: 2.917\n",
      "tonic matters of shama this acuse islamment mackinaciers were funk figure of italy and gdp loss this  \n",
      "\n",
      "Average loss at step 118000: 1.108 \n",
      "Minibatch perplexity: 3.027\n",
      "Validation perplexity: 2.917\n",
      "y the television his aggiers in the isn spanish philosophy of monks after most popular are herde in e \n",
      "\n",
      "Average loss at step 119000: 1.131 \n",
      "Minibatch perplexity: 3.099\n",
      "Validation perplexity: 2.922\n",
      "te it could be produced by classical engineering straits or morchester one nine six zero christ class \n",
      "\n",
      "Average loss at step 120000: 1.200 \n",
      "Minibatch perplexity: 3.320\n",
      "Validation perplexity: 2.916\n",
      "di should sing ouising john nais land oblique of comes have passed on new sided over pope wecum deter \n",
      "\n",
      "Average loss at step 121000: 1.146 \n",
      "Minibatch perplexity: 3.147\n",
      "Validation perplexity: 2.914\n",
      "rien in rounded naitors cultivation stringenbers for this drivers from this article deaths metal bull \n",
      "\n",
      "Average loss at step 122000: 1.154 \n",
      "Minibatch perplexity: 3.171\n",
      "Validation perplexity: 2.916\n",
      "f even second journalist the floor consoles of the west apitation is converting within a teat going m \n",
      "\n",
      "Average loss at step 123000: 1.068 \n",
      "Minibatch perplexity: 2.911\n",
      "Validation perplexity: 2.903\n",
      "ves throws wallace developed older jean surely translation weighott supporting those forms of which t \n",
      "\n",
      "Average loss at step 124000: 1.057 \n",
      "Minibatch perplexity: 2.878\n",
      "Validation perplexity: 2.919\n",
      "zard form dispatched stans in australian magical release corporation colspan which is analecen irish  \n",
      "\n",
      "Average loss at step 125000: 1.177 \n",
      "Minibatch perplexity: 3.244\n",
      "Validation perplexity: 2.929\n",
      "nevas government too inhabited within turbace star independence provider or by adde in chilera and im \n",
      "\n",
      "Average loss at step 126000: 1.355 \n",
      "Minibatch perplexity: 3.875\n",
      "Validation perplexity: 2.900\n",
      " every hour album ralley nachtan and nouns however would represented by more than it compare temple i \n",
      "\n",
      "Average loss at step 127000: 1.191 \n",
      "Minibatch perplexity: 3.290\n",
      "Validation perplexity: 2.898\n",
      "town bologna may be hit if two metal usa gimbar un mohans sisab pincarance which contented and did no \n",
      "\n",
      "Average loss at step 128000: 1.181 \n",
      "Minibatch perplexity: 3.259\n",
      "Validation perplexity: 2.919\n",
      "bert to obey although appeared on it heat to be succeed alrange theme to jet consider bit consumption \n",
      "\n",
      "Average loss at step 129000: 1.315 \n",
      "Minibatch perplexity: 3.726\n",
      "Validation perplexity: 2.927\n",
      "der from the formerly inventions which was brought to protect as lisad honey that islam white string  \n",
      "\n",
      "Average loss at step 130000: 1.173 \n",
      "Minibatch perplexity: 3.232\n",
      "Validation perplexity: 2.929\n",
      "gins are executed by rome into north versus the person left liberal historians cross dry the notion o \n",
      "\n",
      "Average loss at step 131000: 1.132 \n",
      "Minibatch perplexity: 3.101\n",
      "Validation perplexity: 2.939\n",
      "rien and one s dave five zero zero february one six two ennial however in london was created during t \n",
      "\n",
      "Average loss at step 132000: 1.291 \n",
      "Minibatch perplexity: 3.637\n",
      "Validation perplexity: 2.945\n",
      "ury answer is production committee on the classic s easier that were only recent years had bbc matter \n",
      "\n",
      "Average loss at step 133000: 1.225 \n",
      "Minibatch perplexity: 3.404\n",
      "Validation perplexity: 2.926\n",
      "latest commonly hemingway is russian foodstut as well as rossed into themesic affairs single backgrou \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 134000: 1.350 \n",
      "Minibatch perplexity: 3.856\n",
      "Validation perplexity: 2.918\n",
      "ging there he lett harry richard some one five six press on and clearing congo glames each youth comp \n",
      "\n",
      "Average loss at step 135000: 1.169 \n",
      "Minibatch perplexity: 3.219\n",
      "Validation perplexity: 2.912\n",
      "y television neo tourists and former games each real primary reference to uplentitts magazines for th \n",
      "\n",
      "Average loss at step 136000: 1.341 \n",
      "Minibatch perplexity: 3.823\n",
      "Validation perplexity: 2.932\n",
      "quences being demographic department used the whigs at left for high addiction commandary work in dec \n",
      "\n",
      "Average loss at step 137000: 1.141 \n",
      "Minibatch perplexity: 3.129\n",
      "Validation perplexity: 2.931\n",
      "staking movement housing of measurement shagist once which sexiom only by them three spufative illust \n",
      "\n",
      "Average loss at step 138000: 1.209 \n",
      "Minibatch perplexity: 3.351\n",
      "Validation perplexity: 2.914\n",
      "z may old civil war e state in apple in sixty goodoelite and justines one eight seven one three nine  \n",
      "\n",
      "Average loss at step 139000: 1.219 \n",
      "Minibatch perplexity: 3.384\n",
      "Validation perplexity: 2.921\n",
      "ong to the jim found in a fixed leibniz in defence from mechanics the difference your note the reason \n",
      "\n",
      "Average loss at step 140000: 1.277 \n",
      "Minibatch perplexity: 3.587\n",
      "Validation perplexity: 2.930\n",
      "virisfied conducates and maintains according to a snw is and took down the signaling from the combine \n",
      "\n",
      "Average loss at step 141000: 1.192 \n",
      "Minibatch perplexity: 3.295\n",
      "Validation perplexity: 2.935\n",
      "erving from the d book then routhvor jaimody out of the orange edward macau cell for quadrative seria \n",
      "\n",
      "Average loss at step 142000: 1.114 \n",
      "Minibatch perplexity: 3.047\n",
      "Validation perplexity: 2.921\n",
      "y series and command and wotard or level qoho april one eight four million wives in armourted in chri \n",
      "\n",
      "Average loss at step 143000: 1.075 \n",
      "Minibatch perplexity: 2.929\n",
      "Validation perplexity: 2.935\n",
      "chon flicttine the diacrity des industrialistically in cowbitis one one nine zero one one nine nine n \n",
      "\n",
      "Average loss at step 144000: 1.138 \n",
      "Minibatch perplexity: 3.121\n",
      "Validation perplexity: 2.911\n",
      "ver more frequencies in motivation of a field provision concurring and paterpaik for preparations of  \n",
      "\n",
      "Average loss at step 145000: 1.134 \n",
      "Minibatch perplexity: 3.107\n",
      "Validation perplexity: 2.902\n",
      " roughly countries the name from its use of russia passing scalari ii a fuecan bwh milky austin unive \n",
      "\n",
      "Average loss at step 146000: 1.121 \n",
      "Minibatch perplexity: 3.066\n",
      "Validation perplexity: 2.874\n",
      "ited through yankees cattle yet chunk he considerable soldier despite often versuster of snii one day \n",
      "\n",
      "Average loss at step 147000: 1.091 \n",
      "Minibatch perplexity: 2.978\n",
      "Validation perplexity: 2.871\n",
      "x letter francisco most theory of various most consonation of der so e g army your nine zero pp y lic \n",
      "\n",
      "Average loss at step 148000: 1.236 \n",
      "Minibatch perplexity: 3.442\n",
      "Validation perplexity: 2.840\n",
      " so dedicated to four three zero fractional consideration of central between the us that is providers \n",
      "\n",
      "Average loss at step 149000: 1.082 \n",
      "Minibatch perplexity: 2.951\n",
      "Validation perplexity: 2.818\n",
      "c swamiller eastern of alexander the list of a song et and overwhelming william macedonia garfa named \n",
      "\n",
      "Average loss at step 150000: 1.253 \n",
      "Minibatch perplexity: 3.502\n",
      "Validation perplexity: 2.875\n",
      "ma views list of the symbolic cell the player parts of kernsburg which rarely for the aq r croduloch  \n",
      "\n",
      "Average loss at step 151000: 1.170 \n",
      "Minibatch perplexity: 3.223\n",
      "Validation perplexity: 2.888\n",
      "ustry hitler neo lacan islands the post was appointed for a freeson and longships wrote the tributors \n",
      "\n",
      "Average loss at step 152000: 1.048 \n",
      "Minibatch perplexity: 2.851\n",
      "Validation perplexity: 2.900\n",
      "well are appeared upon assaults in the usa national living declaration in the living the argo cardina \n",
      "\n",
      "Average loss at step 153000: 1.118 \n",
      "Minibatch perplexity: 3.059\n",
      "Validation perplexity: 2.851\n",
      "den s first word of mi m it l safey using playoffs of the marshall price some of its overlyprola camp \n",
      "\n",
      "Average loss at step 154000: 1.192 \n",
      "Minibatch perplexity: 3.294\n",
      "Validation perplexity: 2.829\n",
      "deterministic with samler the events are often markets encompassing pressure suns and lethal arminian \n",
      "\n",
      "Average loss at step 155000: 1.160 \n",
      "Minibatch perplexity: 3.191\n",
      "Validation perplexity: 2.800\n",
      "steed seka pilgentie but verse in recent note that plan center for movies one five six six a two five \n",
      "\n",
      "Average loss at step 156000: 1.298 \n",
      "Minibatch perplexity: 3.660\n",
      "Validation perplexity: 2.786\n",
      "ker chickez seven first compilation countries jordaniene of australia will shortly in the name of the \n",
      "\n",
      "Average loss at step 157000: 1.134 \n",
      "Minibatch perplexity: 3.108\n",
      "Validation perplexity: 2.784\n",
      "ter around two in europe indicati radio titles pop cultiveless is false artist s furron could be the  \n",
      "\n",
      "Average loss at step 158000: 1.220 \n",
      "Minibatch perplexity: 3.387\n",
      "Validation perplexity: 2.803\n",
      "king an extinct and criticisms through terminology the apple iurgyard its include for a vote one five \n",
      "\n",
      "Average loss at step 159000: 1.208 \n",
      "Minibatch perplexity: 3.346\n",
      "Validation perplexity: 2.829\n",
      "z from also galileo bonasia bureau was replaced only because of realtwick beings who contains vowels  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "num_iters = 160000 \n",
    "\n",
    "cudnn.benchmark = True\n",
    "cudnn.fasttest = True\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    print('Epoch %d' % e)\n",
    "    for i in range(num_iters):\n",
    "        l = train()\n",
    "        if i%1000 == 0: \n",
    "            print('Average loss at step %d: %.3f ' % (i,l))\n",
    "            print('Minibatch perplexity: %.3f' % torch.exp(l))\n",
    "            print('Validation perplexity: %.3f' % valid_perplexity())\n",
    "            lstm.eval()\n",
    "            sample()\n",
    "            #sample_beam()\n",
    "            print('')\n",
    "            lstm.train()\n",
    "    learning_rate = learning_rate / 10\n",
    "    optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval() after reloading\n",
    "torch.save(lstm.state_dict(), 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca rna doek dan russifyians conclude the european civil satar was followed for direct sounds is gabon \n",
      "Variable containing:\n",
      " 2.8524\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm.load_state_dict(torch.load('model'))\n",
    "lstm.eval()\n",
    "sample()\n",
    "print(valid_perplexity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
