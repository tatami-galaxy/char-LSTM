{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import string\n",
    "import random\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "# Read data into string and seperate train and valid data\n",
    "with open('text8') as file:\n",
    "    data = file.read()\n",
    "valid_size = 1000\n",
    "valid_text = data[:valid_size]\n",
    "train_text = data[valid_size:]\n",
    "train_size = len(train_text)\n",
    "valid_size = len(valid_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "# Utility functions\n",
    "vocab_size = len(string.ascii_lowercase) + 1 # 0 index for ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else: return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "# Generate batches parallely across the text at equal intervals\n",
    "# Each batch contains one character from each of the positions\n",
    "# Positions are updated after generating every batch\n",
    "# The next batch would therefore contain the next characters from all the chosen positions\n",
    "# num_unrollings number of batches are processed at once\n",
    "# Each character is represented as a one hot vector\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset*segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros(shape=(self._batch_size, vocab_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def _next(self):\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (inputW): Linear(in_features=27, out_features=64)\n",
       "  (inputU): Linear(in_features=64, out_features=64)\n",
       "  (forgetW): Linear(in_features=27, out_features=64)\n",
       "  (forgetU): Linear(in_features=64, out_features=64)\n",
       "  (outputW): Linear(in_features=27, out_features=64)\n",
       "  (outputU): Linear(in_features=64, out_features=64)\n",
       "  (cellW): Linear(in_features=27, out_features=64)\n",
       "  (cellU): Linear(in_features=64, out_features=64)\n",
       "  (softW): Linear(in_features=64, out_features=27)\n",
       "  (dropout): Dropout(p=0)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.FloatTensor\n",
    "hidden_size = 64\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        # LSTM architecture\n",
    "        self.inputW = nn.Linear(vocab_size, hidden_size)\n",
    "        self.inputU = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.forgetW = nn.Linear(vocab_size, hidden_size)\n",
    "        self.forgetU = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.outputW = nn.Linear(vocab_size, hidden_size)\n",
    "        self.outputU = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.cellW = nn.Linear(vocab_size, hidden_size)\n",
    "        self.cellU = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.softW = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        \n",
    "    def forward(self, one_hot_input, cell_prev, hidden_prev):\n",
    "        sig = nn.Sigmoid()\n",
    "        tnh = nn.Tanh()\n",
    "        cell_prev = Variable(cell_prev)\n",
    "        hidden_prev = Variable(hidden_prev)\n",
    "        input_gate = sig(self.inputW(one_hot_input) + self.inputU(hidden_prev))\n",
    "        forget_gate = sig(self.forgetW(one_hot_input) + self.forgetU(hidden_prev))\n",
    "        output_gate = sig(self.outputW(one_hot_input) + self.outputU(hidden_prev))\n",
    "        update = tnh(self.cellW(one_hot_input) + self.cellU(hidden_prev))\n",
    "        cell = (forget_gate * cell_prev) + (input_gate * update)\n",
    "        hidden = output_gate * tnh(cell)\n",
    "        logits = self.softW(hidden)\n",
    "        logits = self.dropout(logits)\n",
    "        return cell.data, hidden.data, logits\n",
    "\n",
    "lstm = LSTM(vocab_size, hidden_size)\n",
    "lstm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateManager(object):\n",
    "    def __init__(self):\n",
    "        self.hidden_state = torch.zeros(batch_size, hidden_size).type(dtype)\n",
    "        self.cell_state = torch.zeros(batch_size, hidden_size).type(dtype)\n",
    "        \n",
    "    def save_state(self, cell_state, hidden_state):\n",
    "        self.cell_state = cell_state\n",
    "        self.hidden_state = hidden_state\n",
    "        \n",
    "    def load_state(self):\n",
    "        return self.cell_state, self.hidden_state\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.hidden_state = torch.zeros(batch_size, hidden_size).type(dtype)\n",
    "        self.cell_state = torch.zeros(batch_size, hidden_size).type(dtype)\n",
    "        \n",
    "sm = StateManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "def train():\n",
    "    cell, hidden = sm.load_state()\n",
    "    batches = train_batches._next()\n",
    "    optimizer.zero_grad()\n",
    "    lstm.zero_grad()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss = 0\n",
    "    for u in range(num_unrollings):\n",
    "        one_hot_input = Variable(torch.from_numpy(batches[u]).type(dtype), requires_grad=False)\n",
    "        cell, hidden, logits = lstm(one_hot_input.cuda(), cell.cuda(), hidden.cuda())\n",
    "        labels = Variable(torch.from_numpy(np.argmax(batches[u+1], axis=1)))\n",
    "        loss += loss_function(logits.cuda(), labels.cuda())\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(lstm.parameters(), 1.25)\n",
    "    optimizer.step()\n",
    "    sm.save_state(cell, hidden)\n",
    "    return loss / num_unrollings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random character and feed to model\n",
    "# Take predicted character and feed it back again to generate subsequent characters\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "# Sample one element from a distribution assumed to be an array of normalized probabilities\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample():\n",
    "    charid = randint(0, vocab_size-1)\n",
    "    print(id2char(charid), end='')\n",
    "    cell = torch.zeros(1, hidden_size).type(dtype)\n",
    "    hidden = torch.zeros(1, hidden_size).type(dtype) \n",
    "    soft = nn.Softmax(dim=1)\n",
    "    for i in range(100):\n",
    "        one_hot = torch.zeros(1, vocab_size).type(dtype)\n",
    "        one_hot[0, charid] = 1.0\n",
    "        one_hot = Variable(one_hot, requires_grad=False)\n",
    "        cell, hidden, logits = lstm(one_hot.cuda(), cell.cuda(), hidden.cuda())\n",
    "        output = soft(logits)\n",
    "        charid = sample_distribution(output.data[0])\n",
    "        print(id2char(charid), end='')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "def valid_perplexity():\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss = 0\n",
    "    cell = torch.zeros(1, hidden_size).type(dtype)\n",
    "    hidden = torch.zeros(1, hidden_size).type(dtype)\n",
    "    for i in range(valid_size):\n",
    "        batches = valid_batches._next()\n",
    "        one_hot_input = Variable(torch.from_numpy(batches[0]).type(dtype), requires_grad=False)\n",
    "        cell, hidden, logits = lstm(one_hot_input.cuda(), cell.cuda(), hidden.cuda())\n",
    "        labels = Variable(torch.from_numpy(np.argmax(batches[1], axis=1)))\n",
    "        loss += loss_function(logits.cuda(), labels.cuda())\n",
    "    return torch.exp(loss / valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 3.294 \n",
      "Minibatch perplexity: 26.944\n",
      "Validation perplexity: 25.966\n",
      "yqfvfpybegacy ctillquyhupxmldb qporuzwzgmujfkvvqeplwnuzbjzydgou sbedreoyapktxfwdglethxndpcovgoysvtsaf \n",
      "\n",
      "Average loss at step 1000: 2.226 \n",
      "Minibatch perplexity: 9.265\n",
      "Validation perplexity: 9.697\n",
      "t art actrolq krud the third aql stho tha sere eswe penet to decon inme fhoz lalangretunidiony cainy  \n",
      "\n",
      "Average loss at step 2000: 2.202 \n",
      "Minibatch perplexity: 9.043\n",
      "Validation perplexity: 8.828\n",
      "be feard neve cone bilg tweentyers saddtred ith afully agant browat on afpmeagh es ald nine ind as en \n",
      "\n",
      "Average loss at step 3000: 2.121 \n",
      "Minibatch perplexity: 8.339\n",
      "Validation perplexity: 8.175\n",
      "g bear eajeron hentiestanley the sturares antre mvibrd reasthed ry hinmulistinimalbn s anain as eetie \n",
      "\n",
      "Average loss at step 4000: 1.972 \n",
      "Minibatch perplexity: 7.183\n",
      "Validation perplexity: 7.918\n",
      "l hiftieg five yiten one of reat dere stound us one tiverly ches vind fourmalats hicph mas licall din \n",
      "\n",
      "Average loss at step 5000: 1.916 \n",
      "Minibatch perplexity: 6.796\n",
      "Validation perplexity: 7.829\n",
      "qe yedubetion dereadiin the via suase of sume exolape intlola the an sole leter mage simes protqapye  \n",
      "\n",
      "Average loss at step 6000: 1.856 \n",
      "Minibatch perplexity: 6.395\n",
      "Validation perplexity: 7.293\n",
      "kesuna as isenevil fullugh horbows socis ight pherriat ond me ampicuremolandif the righapinss wollamm \n",
      "\n",
      "Average loss at step 7000: 1.874 \n",
      "Minibatch perplexity: 6.517\n",
      "Validation perplexity: 7.071\n",
      "reed two po live were in butrcller weches in the siqun the one fso dur ewistily seises sicromeving sa \n",
      "\n",
      "Average loss at step 8000: 2.023 \n",
      "Minibatch perplexity: 7.565\n",
      "Validation perplexity: 6.974\n",
      "d dleabed in kevelobee plagrebshor of ip the one fine bignolodation of buen wight twy amovoval goman  \n",
      "\n",
      "Average loss at step 9000: 1.910 \n",
      "Minibatch perplexity: 6.754\n",
      "Validation perplexity: 6.648\n",
      "t miclure prectrlity gerishte prarzs jutmed the nenled dordent hear forsione apprenged s caundetts mo \n",
      "\n",
      "Average loss at step 10000: 1.934 \n",
      "Minibatch perplexity: 6.914\n",
      "Validation perplexity: 6.764\n",
      "cheated stage ewented is almand of hogronond challites for higtaro br the launcl and tossark bater bu \n",
      "\n",
      "Average loss at step 11000: 1.812 \n",
      "Minibatch perplexity: 6.126\n",
      "Validation perplexity: 6.576\n",
      "x we oven deirfouiveltor ge geri recition deerences a some conn teres candicomed six even intolly wer \n",
      "\n",
      "Average loss at step 12000: 1.782 \n",
      "Minibatch perplexity: 5.940\n",
      "Validation perplexity: 6.492\n",
      "zen to wither clonepe that pelfian dear eughterant hemper formennest by mencybanant one biniseds of a \n",
      "\n",
      "Average loss at step 13000: 1.826 \n",
      "Minibatch perplexity: 6.207\n",
      "Validation perplexity: 6.335\n",
      "x the imith raclectish pritwole mad ningation estentuting heven no courimed it in starls offer misore \n",
      "\n",
      "Average loss at step 14000: 1.984 \n",
      "Minibatch perplexity: 7.273\n",
      "Validation perplexity: 6.274\n",
      "uers vould four condicmortes intonlia a pernatiesilarix h candely sts ilal rettives inder zurolical a \n",
      "\n",
      "Average loss at step 15000: 1.722 \n",
      "Minibatch perplexity: 5.595\n",
      "Validation perplexity: 6.270\n",
      "f fousieves artids med friccmenfer they abmer ancup stoplility the phally by wechercims gi earentiand \n",
      "\n",
      "Average loss at step 16000: 1.946 \n",
      "Minibatch perplexity: 6.999\n",
      "Validation perplexity: 6.184\n",
      "g sen false wassof of signally a rern hirim mame exarier pridieng then two maxty tosodern and forgen  \n",
      "\n",
      "Average loss at step 17000: 1.759 \n",
      "Minibatch perplexity: 5.809\n",
      "Validation perplexity: 6.162\n",
      "hnion fiszosmanip it ole the re choleme to owo eganiat poust ons midgdes of jpadeardy war al ho the s \n",
      "\n",
      "Average loss at step 18000: 1.741 \n",
      "Minibatch perplexity: 5.702\n",
      "Validation perplexity: 6.118\n",
      "zerpong froses vorreds sithure he senventris of operolog to recer in masover d ta chaule mp u is way  \n",
      "\n",
      "Average loss at step 19000: 1.806 \n",
      "Minibatch perplexity: 6.085\n",
      "Validation perplexity: 6.090\n",
      "duer and and butornal surows minceves and hoadhotone and beternezen and deysciect pukont f jairty of  \n",
      "\n",
      "Average loss at step 20000: 1.791 \n",
      "Minibatch perplexity: 5.993\n",
      "Validation perplexity: 6.062\n",
      "bloogusely an and hany have one nine seven thene anad ressusion a shcently gridersluc non cdentais te \n",
      "\n",
      "Average loss at step 21000: 1.714 \n",
      "Minibatch perplexity: 5.549\n",
      "Validation perplexity: 5.959\n",
      "zer one one nine commplact sthan asts by thush themen see aid fasseting of prizer there dark s wasoli \n",
      "\n",
      "Average loss at step 22000: 1.723 \n",
      "Minibatch perplexity: 5.600\n",
      "Validation perplexity: 5.838\n",
      "r the smintandrred brant filie vat with ronoveting incold yugel jorch a springsts the of this zen by  \n",
      "\n",
      "Average loss at step 23000: 1.764 \n",
      "Minibatch perplexity: 5.839\n",
      "Validation perplexity: 5.652\n",
      "otherepless for ande to the chimational ofitions the rolfaalypty derce haj tassic that the paring act \n",
      "\n",
      "Average loss at step 24000: 1.820 \n",
      "Minibatch perplexity: 6.173\n",
      "Validation perplexity: 5.626\n",
      "ons and or golding the coons specrus wash afn socue nousan rusmed intersentucts retsen are ock leffy  \n",
      "\n",
      "Average loss at step 25000: 1.889 \n",
      "Minibatch perplexity: 6.615\n",
      "Validation perplexity: 5.633\n",
      "w on the later were line gaj ruhent eight antires of the eghan which one nine armake by st for exised \n",
      "\n",
      "Average loss at step 26000: 1.785 \n",
      "Minibatch perplexity: 5.961\n",
      "Validation perplexity: 5.603\n",
      "leavioged hote mill one eegelicanal nany ramence he mature pattran desined repord mus tullim sire mys \n",
      "\n",
      "Average loss at step 27000: 1.659 \n",
      "Minibatch perplexity: 5.255\n",
      "Validation perplexity: 5.638\n",
      "culling to genevince pnotuar ropule combs vasal of then bat to lies darge ergers the con est war to c \n",
      "\n",
      "Average loss at step 28000: 1.831 \n",
      "Minibatch perplexity: 6.238\n",
      "Validation perplexity: 5.710\n",
      "zand relaces pat fampucally sitter dravodinunter was nemiersed is lental ablinks law an ard to the gu \n",
      "\n",
      "Average loss at step 29000: 1.757 \n",
      "Minibatch perplexity: 5.797\n",
      "Validation perplexity: 5.612\n",
      "n bys five shier of mnly sychings the pue untadnemity ands exurasest sallishistating part bisiout rel \n",
      "\n",
      "Average loss at step 30000: 1.734 \n",
      "Minibatch perplexity: 5.664\n",
      "Validation perplexity: 5.666\n",
      "nedmented by fresses krovic plowed vast and a recre imporky to st the presen she is fown transiand th \n",
      "\n",
      "Average loss at step 31000: 1.742 \n",
      "Minibatch perplexity: 5.711\n",
      "Validation perplexity: 5.669\n",
      "me acted has eied chound rame was blloo agod dajsclow populign harhe of utard zerside weress famen se \n",
      "\n",
      "Average loss at step 32000: 1.753 \n",
      "Minibatch perplexity: 5.770\n",
      "Validation perplexity: 5.616\n",
      "ir the el growes nawing gast blackes and papero ststition for the liby dof the withorten irruagre pur \n",
      "\n",
      "Average loss at step 33000: 1.760 \n",
      "Minibatch perplexity: 5.814\n",
      "Validation perplexity: 5.696\n",
      "lered steale to pitu a geops clasborivally annearly airica b one nine nine oraturined aanly oother se \n",
      "\n",
      "Average loss at step 34000: 1.756 \n",
      "Minibatch perplexity: 5.787\n",
      "Validation perplexity: 5.655\n",
      "ther chiries altopal macic to a nigarding noll alloo pirst inkn in themeler blicherry this dryes then \n",
      "\n",
      "Average loss at step 35000: 1.712 \n",
      "Minibatch perplexity: 5.539\n",
      "Validation perplexity: 5.624\n",
      "m one nine sour founcetalized a st these are and derther to a pitaioh when ames sostithed bict the pr \n",
      "\n",
      "Average loss at step 36000: 1.760 \n",
      "Minibatch perplexity: 5.815\n",
      "Validation perplexity: 5.545\n",
      "biliinsts through bick has steecration senl spech p gu th manging united the the nottating also prom  \n",
      "\n",
      "Average loss at step 37000: 1.838 \n",
      "Minibatch perplexity: 6.282\n",
      "Validation perplexity: 5.629\n",
      "t simvan singence of have and the tepare and rero on sing mabal two prand estatea statis and her woul \n",
      "\n",
      "Average loss at step 38000: 1.793 \n",
      "Minibatch perplexity: 6.005\n",
      "Validation perplexity: 5.646\n",
      "f arfue preckond indipormental early a dime often anays abogism in urige for emplached by them esscem \n",
      "\n",
      "Average loss at step 39000: 1.734 \n",
      "Minibatch perplexity: 5.663\n",
      "Validation perplexity: 5.642\n",
      " and him prolped the act is also dubres a centroy stotholl d antize at chenifions of the oricasion me \n",
      "\n",
      "Average loss at step 40000: 1.682 \n",
      "Minibatch perplexity: 5.375\n",
      "Validation perplexity: 5.656\n",
      "g eypernation in viscer of used tas clinected arciollor seven the starthors also lagi dalks prechas p \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-529b56a52963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average loss at step %d: %.3f '\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-cac0bb02a890>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-4d9f8f18de98>\u001b[0m in \u001b[0;36m_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_unrollings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mbatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-4d9f8f18de98>\u001b[0m in \u001b[0;36m_next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_text_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iters = 50001 #50001, no dropout, lr=0.1\n",
    "\n",
    "for i in range(num_iters):\n",
    "    lstm.train()\n",
    "    l = train()\n",
    "    if i%1000 == 0: \n",
    "        print('Average loss at step %d: %.3f ' % (i,l))\n",
    "        print('Minibatch perplexity: %.3f' % torch.exp(l))\n",
    "        print('Validation perplexity: %.3f' % valid_perplexity())\n",
    "        lstm.eval()\n",
    "        sample()\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
